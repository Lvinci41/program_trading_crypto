{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import matplotlib as plot\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import itertools as it\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from keras.utils import to_categorical\n",
    "#from keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "\n",
    "enc = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "BTC_df = pd.read_csv('20250202-20170908_BTC-USDT_1D_okx_ohlc_M.csv')\n",
    "BNB_df = pd.read_csv('20250127-20170907_BNB-USDT_1H_ohlc_M.csv')\n",
    "DOGE_df = pd.read_csv('20250202-20170908_DOGE-USDT_1D_okx_ohlc_M.csv')\n",
    "XRP_df = pd.read_csv('20250202-20170908_XRP-USDT_1D_okx_ohlc_M.csv')\n",
    "#DOT_df = pd.read_csv('2025-01-27-2017-09-07_DOT-USDT_1H_ohlc_M.csv')\n",
    "#SHIB_df = pd.read_csv('20250202-20170908_SHIB-USDT_1D_okx_ohlc_M.csv')\n",
    "#df['Date'] = pd.to_datetime(df['timestamp'], format='%m/%d/%Y %I:%M:%S %p')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RSI CALC\n",
    "def get_up_or_down(df, period):\n",
    "    for i in range(len(df)):\n",
    "        if i > 0:\n",
    "            if df.iloc[i]['close'] >= df.iloc[i-1]['close']:\n",
    "                df.at[i, 'gain_'+str(period)] = df.iloc[i]['close'] - df.iloc[i-1]['close']\n",
    "                df.at[i, 'loss_'+str(period)] = 0\n",
    "            elif df.iloc[i]['close'] < df.iloc[i-1]['close']:\n",
    "                df.at[i, 'loss_'+str(period)] = df.iloc[i-1]['close'] - df.iloc[i]['close']\n",
    "                df.at[i, 'gain_'+str(period)] = 0\n",
    "            else:\n",
    "                df.at[i, 'gain_'+str(period)] = 0\n",
    "                df.at[i, 'loss_'+str(period)] = 0\n",
    "    return df\n",
    "\n",
    "def get_up_or_down_bin(df, offset):\n",
    "    for i in range(len(df)):\n",
    "        if i > 0:\n",
    "            if df.iloc[i]['close'] >= df.iloc[i-offset]['close']:\n",
    "                df.at[i, 'updown_'+str(offset)] = 1\n",
    "            elif df.iloc[i]['close'] < df.iloc[i-offset]['close']:\n",
    "                df.at[i, 'updown_'+str(offset)] = -1                \n",
    "            else:\n",
    "                df.at[i, 'updown_'+str(offset)] = 0\n",
    "    return df\n",
    "  \n",
    "def get_relative_strength_index(df, period):\n",
    "    try:\n",
    "        df['Date'] = pd.to_datetime(df['timestamp'])\n",
    "    except:\n",
    "        df['Date'] = pd.to_datetime(df['timestamp'], format='%m/%d/%Y %I:%M:%S %p', errors='coerce')\n",
    "    #df['Date'] = pd.to_datetime(df['timestamp'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "    #df['Date'] = pd.to_datetime(df['timestamp'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "    #df['Date'] = pd.to_datetime(df['timestamp'], format='%m/%d/%Y %I:%M:%S %p', errors='coerce')\n",
    "\n",
    "    df.set_index(df['Date'])\n",
    "    df = get_up_or_down(df, period)\n",
    "    return df\n",
    "\n",
    "def get_average_gains(df, period):\n",
    "    for i in range(len(df)):\n",
    "        n, up, down = 0, 0, 0\n",
    "        if i == period:\n",
    "            while n < period:\n",
    "                if df.iloc[i-n]['gain_'+str(period)] > 0:\n",
    "                    up += df.iloc[i-n]['gain_'+str(period)]\n",
    "                elif df.iloc[i-n]['loss_'+str(period)] > 0:\n",
    "                    down += df.iloc[i-n]['loss_'+str(period)]\n",
    "                else:\n",
    "                    up += 0\n",
    "                    down += 0\n",
    "                n += 1\n",
    "            df.at[i, 'ag_'+str(period)] = up/period\n",
    "            df.at[i, 'al_'+str(period)] = down/period\n",
    "        elif i > period:\n",
    "            df.at[i, 'ag_'+str(period)] = (df.iloc[i-1]['ag_'+str(period)] * (period - 1) + df.iloc[i]['gain_'+str(period)])/period\n",
    "            df.at[i, 'al_'+str(period)] = (df.iloc[i-1]['al_'+str(period)] * (period - 1) + df.iloc[i]['loss_'+str(period)])/period\n",
    "            df['ag_'+str(period)] = df['ag_'+str(period)].fillna(0)\n",
    "            df['al_'+str(period)] = df['al_'+str(period)].fillna(0)\n",
    "    return df\n",
    "\n",
    "def get_relative_strength(df, period):\n",
    "    df = get_relative_strength_index(df,period)\n",
    "    df = get_average_gains(df, period)\n",
    "    rs_col = f'rs_{period}' #added\n",
    "    rsi_col = f'rsi_{period}'#added\n",
    "    df[rs_col] = np.nan#added\n",
    "    df[rsi_col] = np.nan    #added\n",
    "    #for i in range(len(df)):\n",
    "    '''\n",
    "    if i >= period:\n",
    "            df.at[i, 'rs_'+str(period)] = df.iloc[i]['ag_'+str(period)]/df.iloc[i]['al_'+str(period)]\n",
    "            df.at[i, 'rsi_'+str(period)] = (100-(100/(1+df.iloc[i]['rs_'+str(period)])))\n",
    "    '''\n",
    "    for i in range(len(df)):\n",
    "        if i >= period and df.iloc[i]['al_'+str(period)] != 0:\n",
    "            df.at[i, rs_col] = df.iloc[i]['ag_'+str(period)] / df.iloc[i]['al_'+str(period)]\n",
    "            df.at[i, rsi_col] = 100 - (100 / (1 + df.at[i, rs_col]))\n",
    "        elif i >= period:\n",
    "            df.at[i, rs_col] = 0\n",
    "            df.at[i, rsi_col] = 100\n",
    "\n",
    "    return df\n",
    "\n",
    "##MONEY FLOW\n",
    "def get_typical_price(high, low, close):\n",
    "    typical_price = (high+low+close/3)\n",
    "    return typical_price\n",
    "\n",
    "def get_raw_money_flow(typical_price, volume):\n",
    "    money_flow = typical_price * volume\n",
    "    return money_flow\n",
    "\n",
    "def get_money_flow_ratio(money_flow, window=14):\n",
    "    signal = np.where(money_flow > money_flow.shift(1), 1, np.where(money_flow < money_flow.shift(1), -1, 0))\n",
    "    money_flow_s = money_flow * signal\n",
    "    \n",
    "    money_flow_positive = money_flow_s.rolling(window).apply(lambda x: np.sum(np.where(x >= 0.0, x, 0.0)), raw=True)\n",
    "    money_flow_negative = abs(money_flow_s.rolling(window).apply(lambda x: np.sum(np.where(x < 0.0, x, 0.0)), raw=True))\n",
    "    \n",
    "    money_flow_ratio = money_flow_positive / money_flow_negative\n",
    "    \n",
    "    return money_flow_ratio\n",
    "\n",
    "def get_money_flow_index(money_flow_ratio):\n",
    "    money_flow_index = 100. - 100./(1. + money_flow_ratio)\n",
    "    return money_flow_index\n",
    "\n",
    "def money_flow_index(high, low, close, volume, window=14):\n",
    "    mfr = get_money_flow_ratio((high+low+close/3) * volume, window)\n",
    "    mfi = 100. - 100./(1. + mfr)\n",
    "    return mfi\n",
    "\n",
    "#Choppiness index\n",
    "def get_ci(high, low, close, lookback):\n",
    "    tr1 = pd.DataFrame(high - low).rename(columns = {0:'tr1'})\n",
    "    tr2 = pd.DataFrame(abs(high - close.shift(1))).rename(columns = {0:'tr2'})\n",
    "    tr3 = pd.DataFrame(abs(low - close.shift(1))).rename(columns = {0:'tr3'})\n",
    "    frames = [tr1, tr2, tr3]\n",
    "    tr = pd.concat(frames, axis = 1, join = 'inner').dropna().max(axis = 1)\n",
    "    atr = tr.rolling(1).mean()\n",
    "    highh = high.rolling(lookback).max()\n",
    "    lowl = low.rolling(lookback).min()\n",
    "    ci = 100 * np.log10((atr.rolling(lookback).sum()) / (highh - lowl)) / np.log10(lookback)\n",
    "    return ci\n",
    "\n",
    "#Feature Extraction\n",
    "def feature_extraction(df, time_steps_arr):\n",
    "    df['range'] = df['high'] - df['low']\n",
    "    df['range%'] = df['range']/df['close']\n",
    "    df['obv'] = (np.sign(df['close'].diff()) * df['volume_ccy']).fillna(0).cumsum()    \n",
    "    df['return'] = df['close'].pct_change() \n",
    "    \n",
    "    features_list = ['range', 'range%', 'obv', 'return']\n",
    "    for time in time_steps_arr:\n",
    "        #df['return_'+str(time)] = (df.close / df.close.shift(time)) - 1\n",
    "        df['return_'+str(time)] = (df['close'] / df['close'].shift(time)) - 1\n",
    "        df = get_relative_strength(df, time)\n",
    "        df = get_up_or_down_bin(df, time)\n",
    "        df['std_'+str(time)] = df['return_'+str(time)].rolling(time).std()\n",
    "        df['ma_'+str(time)] = df['close'].rolling(time).mean()\n",
    "        df['mfi_'+str(time)]= money_flow_index(df['high'], df['low'], df['close'], df['volume_ccy'], time)\n",
    "        df['ma_'+str(time)] = df['close'].rolling(time).mean()\n",
    "        df['avgvolm_'+str(time)] = df['volume_ccy'].rolling(time).mean()\n",
    "        df['avgvolty_'+str(time)] = df['std_'+str(time)].rolling(time).mean()\n",
    "        df['rtrend_'+str(time)] = df['updown_'+str(time)].rolling(time).sum()\n",
    "        df['ci_'+str(time)] = get_ci(df['high'], df['low'], df['close'], time)\n",
    "        \n",
    "        features_list.extend(['return_' + str(time), 'rs_' + str(time), 'updown_' + str(time),'std_' + str(time), 'ma_' + str(time), 'mfi_' + str(time),'avgvolm_' + str(time), 'avgvolty_' + str(time), 'rtrend_' + str(time),'ci_' + str(time)])\n",
    "    return df, features_list\n",
    "\n",
    "def convert_markov_states(df):\n",
    "    '''\n",
    "    for column in df.columns:\n",
    "        for item in column:\n",
    "            if item == \"dtrending\" or item == \"down\":\n",
    "                item = 1                \n",
    "            elif item == \"rangebound\" or item == \"flat\":\n",
    "                item = 2\n",
    "            elif item == \"utrending\" or item == \"up\":\n",
    "                item = 3                \n",
    "            else:\n",
    "                item == 0\n",
    "                \n",
    "    '''\n",
    "    for col_name, col_data in df.iteritems():\n",
    "        col_data = col_data.str.strip().str.lower()\n",
    "        for item in col_data:\n",
    "            if str(item) == \"dtrending\" or item == \"down\":\n",
    "                item = 1                \n",
    "            elif item == \"rangebound\" or item == \"flat\":\n",
    "                item = 2\n",
    "            elif item == \"utrending\" or item == \"up\":\n",
    "                item = 3                \n",
    "            else:\n",
    "                item == 0        \n",
    "    return df\n",
    "                \n",
    "def training_states_shift(df, time_steps_arr):\n",
    "    for time in time_steps_arr:\n",
    "        df['mStateSt_'+str(time)] = df.m_st.shift(time)\n",
    "        df['mStateMid_'+str(time)] = df.m_mt.shift(time)\n",
    "        df['mStateLt_'+str(time)] = df.m_lt.shift(time)\n",
    "        \n",
    "        training_list = []\n",
    "        training_list.append('mStateSt_'+str(time))\n",
    "        training_list.append('mStateMid_'+str(time))\n",
    "        training_list.append('mStateLt_'+str(time))\n",
    "    return df, training_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.at[i, 'loss_'+str(period)] = 0\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:53: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.at[i, 'ag_'+str(period)] = up/period\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.at[i, 'al_'+str(period)] = down/period\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[rs_col] = np.nan#added\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[rsi_col] = np.nan    #added\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.at[i, 'updown_'+str(offset)] = -1\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:140: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['std_'+str(time)] = df['return_'+str(time)].rolling(time).std()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['ma_'+str(time)] = df['close'].rolling(time).mean()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:142: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mfi_'+str(time)]= money_flow_index(df['high'], df['low'], df['close'], df['volume_ccy'], time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['avgvolm_'+str(time)] = df['volume_ccy'].rolling(time).mean()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['avgvolty_'+str(time)] = df['std_'+str(time)].rolling(time).mean()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['rtrend_'+str(time)] = df['updown_'+str(time)].rolling(time).sum()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['ci_'+str(time)] = get_ci(df['high'], df['low'], df['close'], time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateSt_'+str(time)] = df.m_st.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateMid_'+str(time)] = df.m_mt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateLt_'+str(time)] = df.m_lt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateSt_'+str(time)] = df.m_st.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateMid_'+str(time)] = df.m_mt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateLt_'+str(time)] = df.m_lt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateSt_'+str(time)] = df.m_st.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateMid_'+str(time)] = df.m_mt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateLt_'+str(time)] = df.m_lt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateSt_'+str(time)] = df.m_st.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateMid_'+str(time)] = df.m_mt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateLt_'+str(time)] = df.m_lt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateSt_'+str(time)] = df.m_st.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateMid_'+str(time)] = df.m_mt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateLt_'+str(time)] = df.m_lt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateSt_'+str(time)] = df.m_st.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateMid_'+str(time)] = df.m_mt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateLt_'+str(time)] = df.m_lt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateSt_'+str(time)] = df.m_st.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateMid_'+str(time)] = df.m_mt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateLt_'+str(time)] = df.m_lt.shift(time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.at[i, 'gain_'+str(period)] = 0\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:53: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.at[i, 'ag_'+str(period)] = up/period\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.at[i, 'al_'+str(period)] = down/period\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[rs_col] = np.nan#added\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[rsi_col] = np.nan    #added\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.at[i, 'updown_'+str(offset)] = -1\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:140: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['std_'+str(time)] = df['return_'+str(time)].rolling(time).std()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['ma_'+str(time)] = df['close'].rolling(time).mean()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:142: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mfi_'+str(time)]= money_flow_index(df['high'], df['low'], df['close'], df['volume_ccy'], time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['avgvolm_'+str(time)] = df['volume_ccy'].rolling(time).mean()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['avgvolty_'+str(time)] = df['std_'+str(time)].rolling(time).mean()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['rtrend_'+str(time)] = df['updown_'+str(time)].rolling(time).sum()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['ci_'+str(time)] = get_ci(df['high'], df['low'], df['close'], time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateSt_'+str(time)] = df.m_st.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateMid_'+str(time)] = df.m_mt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateLt_'+str(time)] = df.m_lt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateSt_'+str(time)] = df.m_st.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateMid_'+str(time)] = df.m_mt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateLt_'+str(time)] = df.m_lt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateSt_'+str(time)] = df.m_st.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateMid_'+str(time)] = df.m_mt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateLt_'+str(time)] = df.m_lt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateSt_'+str(time)] = df.m_st.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateMid_'+str(time)] = df.m_mt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateLt_'+str(time)] = df.m_lt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateSt_'+str(time)] = df.m_st.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateMid_'+str(time)] = df.m_mt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateLt_'+str(time)] = df.m_lt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateSt_'+str(time)] = df.m_st.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateMid_'+str(time)] = df.m_mt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateLt_'+str(time)] = df.m_lt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateSt_'+str(time)] = df.m_st.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateMid_'+str(time)] = df.m_mt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateLt_'+str(time)] = df.m_lt.shift(time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:53: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.at[i, 'ag_'+str(period)] = up/period\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.at[i, 'al_'+str(period)] = down/period\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[rs_col] = np.nan#added\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[rsi_col] = np.nan    #added\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.at[i, 'updown_'+str(offset)] = -1\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:140: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['std_'+str(time)] = df['return_'+str(time)].rolling(time).std()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['ma_'+str(time)] = df['close'].rolling(time).mean()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:142: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mfi_'+str(time)]= money_flow_index(df['high'], df['low'], df['close'], df['volume_ccy'], time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['avgvolm_'+str(time)] = df['volume_ccy'].rolling(time).mean()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['avgvolty_'+str(time)] = df['std_'+str(time)].rolling(time).mean()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['rtrend_'+str(time)] = df['updown_'+str(time)].rolling(time).sum()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['ci_'+str(time)] = get_ci(df['high'], df['low'], df['close'], time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateSt_'+str(time)] = df.m_st.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateMid_'+str(time)] = df.m_mt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateLt_'+str(time)] = df.m_lt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateSt_'+str(time)] = df.m_st.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateMid_'+str(time)] = df.m_mt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateLt_'+str(time)] = df.m_lt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateSt_'+str(time)] = df.m_st.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateMid_'+str(time)] = df.m_mt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateLt_'+str(time)] = df.m_lt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateSt_'+str(time)] = df.m_st.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateMid_'+str(time)] = df.m_mt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateLt_'+str(time)] = df.m_lt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateSt_'+str(time)] = df.m_st.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateMid_'+str(time)] = df.m_mt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateLt_'+str(time)] = df.m_lt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateSt_'+str(time)] = df.m_st.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateMid_'+str(time)] = df.m_mt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateLt_'+str(time)] = df.m_lt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateSt_'+str(time)] = df.m_st.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateMid_'+str(time)] = df.m_mt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateLt_'+str(time)] = df.m_lt.shift(time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.at[i, 'loss_'+str(period)] = 0\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:53: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.at[i, 'ag_'+str(period)] = up/period\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.at[i, 'al_'+str(period)] = down/period\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[rs_col] = np.nan#added\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[rsi_col] = np.nan    #added\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.at[i, 'updown_'+str(offset)] = 0\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:140: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['std_'+str(time)] = df['return_'+str(time)].rolling(time).std()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['ma_'+str(time)] = df['close'].rolling(time).mean()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:142: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mfi_'+str(time)]= money_flow_index(df['high'], df['low'], df['close'], df['volume_ccy'], time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['avgvolm_'+str(time)] = df['volume_ccy'].rolling(time).mean()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['avgvolty_'+str(time)] = df['std_'+str(time)].rolling(time).mean()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['rtrend_'+str(time)] = df['updown_'+str(time)].rolling(time).sum()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['ci_'+str(time)] = get_ci(df['high'], df['low'], df['close'], time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateSt_'+str(time)] = df.m_st.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateMid_'+str(time)] = df.m_mt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateLt_'+str(time)] = df.m_lt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateSt_'+str(time)] = df.m_st.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateMid_'+str(time)] = df.m_mt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateLt_'+str(time)] = df.m_lt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateSt_'+str(time)] = df.m_st.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateMid_'+str(time)] = df.m_mt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateLt_'+str(time)] = df.m_lt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateSt_'+str(time)] = df.m_st.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateMid_'+str(time)] = df.m_mt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateLt_'+str(time)] = df.m_lt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateSt_'+str(time)] = df.m_st.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateMid_'+str(time)] = df.m_mt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateLt_'+str(time)] = df.m_lt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateSt_'+str(time)] = df.m_st.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateMid_'+str(time)] = df.m_mt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateLt_'+str(time)] = df.m_lt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateSt_'+str(time)] = df.m_st.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateMid_'+str(time)] = df.m_mt.shift(time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1701523453.py:156: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mStateLt_'+str(time)] = df.m_lt.shift(time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nstack_df = BTC_df.append(XRP_df)\\nstack_df = stack_df.append(BNB_df)\\nstack_df = stack_df.append(DOGE_df)\\n'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BTC_df.columns = BTC_df.columns.str.strip().str.lower()\n",
    "XRP_df.columns = XRP_df.columns.str.strip().str.lower()\n",
    "BNB_df.columns = BNB_df.columns.str.strip().str.lower()\n",
    "DOGE_df.columns = DOGE_df.columns.str.strip().str.lower()\n",
    "\n",
    "'''\n",
    "BTC_df['markov_mt'] = BTC_df['m_mt'].astype('str') \n",
    "BTC_df['markov_mt'] = enc.fit_transform(BTC_df['m_mt'])\n",
    "\n",
    "BTC_df['markov_st'] = BTC_df['m_st'].astype('str')\n",
    "BTC_df['markov_st'] = enc.fit_transform(BTC_df['m_st'])\n",
    "\n",
    "BTC_df['markov_lt'] = BTC_df['m_lt'].astype('str')\n",
    "BTC_df['markov_lt'] = enc.fit_transform(BTC_df['m_lt'])\n",
    "\n",
    "\n",
    "XRP_df['markov_mt'] = XRP_df['m_mt'].astype('str') \n",
    "XRP_df['markov_mt'] = enc.fit_transform(XRP_df['m_mt'])\n",
    "\n",
    "XRP_df['markov_st'] = XRP_df['m_st'].astype('str')\n",
    "XRP_df['markov_st'] = enc.fit_transform(XRP_df['m_st'])\n",
    "\n",
    "XRP_df['markov_lt'] = XRP_df['m_lt'].astype('str')\n",
    "XRP_df['markov_lt'] = enc.fit_transform(XRP_df['m_lt'])\n",
    "\n",
    "\n",
    "BNB_df['markov_mt'] = BNB_df['m_mt'].astype('str') \n",
    "BNB_df['markov_mt'] = enc.fit_transform(BNB_df['m_mt'])\n",
    "\n",
    "BNB_df['markov_st'] = BNB_df['m_st'].astype('str')\n",
    "BNB_df['markov_st'] = enc.fit_transform(BNB_df['m_st'])\n",
    "\n",
    "BNB_df['markov_lt'] = BNB_df['m_lt'].astype('str')\n",
    "BNB_df['markov_lt'] = enc.fit_transform(BNB_df['m_lt'])\n",
    "\n",
    "\n",
    "DOGE_df['markov_mt'] = DOGE_df['m_mt'].astype('str') \n",
    "DOGE_df['markov_mt'] = enc.fit_transform(DOGE_df['m_mt'])\n",
    "\n",
    "DOGE_df['markov_st'] = DOGE_df['m_st'].astype('str')\n",
    "DOGE_df['markov_st'] = enc.fit_transform(DOGE_df['m_st'])\n",
    "\n",
    "DOGE_df['markov_lt'] = DOGE_df['m_lt'].astype('str')\n",
    "DOGE_df['markov_lt'] = enc.fit_transform(DOGE_df['m_lt'])\n",
    "'''\n",
    "\n",
    "#time_periods = (2,3,4,5,6,7,8,9,10,12,15,17,20,25,30,35,40,45,50,60,70,80,90,100,150,200)\n",
    "#time_periods = (2,3,5,7,10,14,19,25,35,50,100,200)\n",
    "#time_periods = (2,15,50,100)\n",
    "time_periods = (2,5,10,15,25,50,100)\n",
    "#time_periods = (2,15)\n",
    "features_list = []\n",
    "BTC_df, features_list = feature_extraction(BTC_df, time_periods)\n",
    "BTC_df, training_list = training_states_shift(BTC_df, time_periods)\n",
    "\n",
    "XRP_df = feature_extraction(XRP_df, time_periods)[0]\n",
    "XRP_df = training_states_shift(XRP_df, time_periods)[0]\n",
    "\n",
    "BNB_df = feature_extraction(BNB_df, time_periods)[0]\n",
    "BNB_df = training_states_shift(BNB_df, time_periods)[0]\n",
    "\n",
    "DOGE_df = feature_extraction(DOGE_df, time_periods)[0]\n",
    "DOGE_df = training_states_shift(DOGE_df, time_periods)[0]\n",
    "\n",
    "#print(features_list)\n",
    "\n",
    "'''\n",
    "stack_df = BTC_df.append(XRP_df)\n",
    "stack_df = stack_df.append(BNB_df)\n",
    "stack_df = stack_df.append(DOGE_df)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['timestamp', 'open', 'high', 'low', 'close', 'volume', 'volume_ccy', 'volccyquote', 'm_mt', 'm_st', 'm_lt', 'm_mt_up', 'm_mt_down', 'm_mt_flat', 'm_st_up', 'm_st_down', 'm_st_flat', 'm_lt_up', 'm_lt_down', 'm_lt_flat', 'range', 'range%', 'obv', 'return', 'return_2', 'Date', 'loss_2', 'gain_2', 'ag_2', 'al_2', 'rs_2', 'rsi_2', 'updown_2', 'std_2', 'ma_2', 'mfi_2', 'avgvolm_2', 'avgvolty_2', 'rtrend_2', 'ci_2', 'return_5', 'loss_5', 'gain_5', 'ag_5', 'al_5', 'rs_5', 'rsi_5', 'updown_5', 'std_5', 'ma_5', 'mfi_5', 'avgvolm_5', 'avgvolty_5', 'rtrend_5', 'ci_5', 'return_10', 'loss_10', 'gain_10', 'ag_10', 'al_10', 'rs_10', 'rsi_10', 'updown_10', 'std_10', 'ma_10', 'mfi_10', 'avgvolm_10', 'avgvolty_10', 'rtrend_10', 'ci_10', 'return_15', 'loss_15', 'gain_15', 'ag_15', 'al_15', 'rs_15', 'rsi_15', 'updown_15', 'std_15', 'ma_15', 'mfi_15', 'avgvolm_15', 'avgvolty_15', 'rtrend_15', 'ci_15', 'return_25', 'loss_25', 'gain_25', 'ag_25', 'al_25', 'rs_25', 'rsi_25', 'updown_25', 'std_25', 'ma_25', 'mfi_25', 'avgvolm_25', 'avgvolty_25', 'rtrend_25', 'ci_25', 'return_50', 'loss_50', 'gain_50', 'ag_50', 'al_50', 'rs_50', 'rsi_50', 'updown_50', 'std_50', 'ma_50', 'mfi_50', 'avgvolm_50', 'avgvolty_50', 'rtrend_50', 'ci_50', 'return_100', 'loss_100', 'gain_100', 'ag_100', 'al_100', 'rs_100', 'rsi_100', 'updown_100', 'std_100', 'ma_100', 'mfi_100', 'avgvolm_100', 'avgvolty_100', 'rtrend_100', 'ci_100', 'mStateSt_2', 'mStateMid_2', 'mStateLt_2', 'mStateSt_5', 'mStateMid_5', 'mStateLt_5', 'mStateSt_10', 'mStateMid_10', 'mStateLt_10', 'mStateSt_15', 'mStateMid_15', 'mStateLt_15', 'mStateSt_25', 'mStateMid_25', 'mStateLt_25', 'mStateSt_50', 'mStateMid_50', 'mStateLt_50', 'mStateSt_100', 'mStateMid_100', 'mStateLt_100']\n",
      "['timestamp', 'open', 'high', 'low', 'close', 'volume', 'volume_ccy', 'volccyquote', 'm_mt', 'm_st', 'm_lt', 'm_mt_up', 'm_mt_down', 'm_mt_flat', 'm_st_up', 'm_st_down', 'm_st_flat', 'm_lt_up', 'm_lt_down', 'm_lt_flat', 'range', 'range%', 'obv', 'return', 'return_2', 'Date', 'gain_2', 'loss_2', 'ag_2', 'al_2', 'rs_2', 'rsi_2', 'updown_2', 'std_2', 'ma_2', 'mfi_2', 'avgvolm_2', 'avgvolty_2', 'rtrend_2', 'ci_2', 'return_5', 'gain_5', 'loss_5', 'ag_5', 'al_5', 'rs_5', 'rsi_5', 'updown_5', 'std_5', 'ma_5', 'mfi_5', 'avgvolm_5', 'avgvolty_5', 'rtrend_5', 'ci_5', 'return_10', 'gain_10', 'loss_10', 'ag_10', 'al_10', 'rs_10', 'rsi_10', 'updown_10', 'std_10', 'ma_10', 'mfi_10', 'avgvolm_10', 'avgvolty_10', 'rtrend_10', 'ci_10', 'return_15', 'gain_15', 'loss_15', 'ag_15', 'al_15', 'rs_15', 'rsi_15', 'updown_15', 'std_15', 'ma_15', 'mfi_15', 'avgvolm_15', 'avgvolty_15', 'rtrend_15', 'ci_15', 'return_25', 'gain_25', 'loss_25', 'ag_25', 'al_25', 'rs_25', 'rsi_25', 'updown_25', 'std_25', 'ma_25', 'mfi_25', 'avgvolm_25', 'avgvolty_25', 'rtrend_25', 'ci_25', 'return_50', 'gain_50', 'loss_50', 'ag_50', 'al_50', 'rs_50', 'rsi_50', 'updown_50', 'std_50', 'ma_50', 'mfi_50', 'avgvolm_50', 'avgvolty_50', 'rtrend_50', 'ci_50', 'return_100', 'gain_100', 'loss_100', 'ag_100', 'al_100', 'rs_100', 'rsi_100', 'updown_100', 'std_100', 'ma_100', 'mfi_100', 'avgvolm_100', 'avgvolty_100', 'rtrend_100', 'ci_100', 'mStateSt_2', 'mStateMid_2', 'mStateLt_2', 'mStateSt_5', 'mStateMid_5', 'mStateLt_5', 'mStateSt_10', 'mStateMid_10', 'mStateLt_10', 'mStateSt_15', 'mStateMid_15', 'mStateLt_15', 'mStateSt_25', 'mStateMid_25', 'mStateLt_25', 'mStateSt_50', 'mStateMid_50', 'mStateLt_50', 'mStateSt_100', 'mStateMid_100', 'mStateLt_100']\n",
      "['timestamp', 'open', 'high', 'low', 'close', 'volume', 'volume_ccy', 'volccyquote', 'm_mt', 'm_st', 'm_lt', 'm_mt_up', 'm_mt_down', 'm_mt_flat', 'm_st_up', 'm_st_down', 'm_st_flat', 'm_lt_up', 'm_lt_down', 'm_lt_flat', 'range', 'range%', 'obv', 'return', 'return_2', 'Date', 'loss_2', 'gain_2', 'ag_2', 'al_2', 'rs_2', 'rsi_2', 'updown_2', 'std_2', 'ma_2', 'mfi_2', 'avgvolm_2', 'avgvolty_2', 'rtrend_2', 'ci_2', 'return_5', 'loss_5', 'gain_5', 'ag_5', 'al_5', 'rs_5', 'rsi_5', 'updown_5', 'std_5', 'ma_5', 'mfi_5', 'avgvolm_5', 'avgvolty_5', 'rtrend_5', 'ci_5', 'return_10', 'loss_10', 'gain_10', 'ag_10', 'al_10', 'rs_10', 'rsi_10', 'updown_10', 'std_10', 'ma_10', 'mfi_10', 'avgvolm_10', 'avgvolty_10', 'rtrend_10', 'ci_10', 'return_15', 'loss_15', 'gain_15', 'ag_15', 'al_15', 'rs_15', 'rsi_15', 'updown_15', 'std_15', 'ma_15', 'mfi_15', 'avgvolm_15', 'avgvolty_15', 'rtrend_15', 'ci_15', 'return_25', 'loss_25', 'gain_25', 'ag_25', 'al_25', 'rs_25', 'rsi_25', 'updown_25', 'std_25', 'ma_25', 'mfi_25', 'avgvolm_25', 'avgvolty_25', 'rtrend_25', 'ci_25', 'return_50', 'loss_50', 'gain_50', 'ag_50', 'al_50', 'rs_50', 'rsi_50', 'updown_50', 'std_50', 'ma_50', 'mfi_50', 'avgvolm_50', 'avgvolty_50', 'rtrend_50', 'ci_50', 'return_100', 'loss_100', 'gain_100', 'ag_100', 'al_100', 'rs_100', 'rsi_100', 'updown_100', 'std_100', 'ma_100', 'mfi_100', 'avgvolm_100', 'avgvolty_100', 'rtrend_100', 'ci_100', 'mStateSt_2', 'mStateMid_2', 'mStateLt_2', 'mStateSt_5', 'mStateMid_5', 'mStateLt_5', 'mStateSt_10', 'mStateMid_10', 'mStateLt_10', 'mStateSt_15', 'mStateMid_15', 'mStateLt_15', 'mStateSt_25', 'mStateMid_25', 'mStateLt_25', 'mStateSt_50', 'mStateMid_50', 'mStateLt_50', 'mStateSt_100', 'mStateMid_100', 'mStateLt_100']\n",
      "['timestamp', 'open', 'high', 'low', 'close', 'volume', 'volume_ccy', 'volccyquote', 'm_mt', 'm_st', 'm_lt', 'm_mt_up', 'm_mt_down', 'm_mt_flat', 'm_st_up', 'm_st_down', 'm_st_flat', 'm_lt_up', 'm_lt_down', 'm_lt_flat', 'range', 'range%', 'obv', 'return', 'return_2', 'Date', 'gain_2', 'loss_2', 'ag_2', 'al_2', 'rs_2', 'rsi_2', 'updown_2', 'std_2', 'ma_2', 'mfi_2', 'avgvolm_2', 'avgvolty_2', 'rtrend_2', 'ci_2', 'return_5', 'gain_5', 'loss_5', 'ag_5', 'al_5', 'rs_5', 'rsi_5', 'updown_5', 'std_5', 'ma_5', 'mfi_5', 'avgvolm_5', 'avgvolty_5', 'rtrend_5', 'ci_5', 'return_10', 'gain_10', 'loss_10', 'ag_10', 'al_10', 'rs_10', 'rsi_10', 'updown_10', 'std_10', 'ma_10', 'mfi_10', 'avgvolm_10', 'avgvolty_10', 'rtrend_10', 'ci_10', 'return_15', 'gain_15', 'loss_15', 'ag_15', 'al_15', 'rs_15', 'rsi_15', 'updown_15', 'std_15', 'ma_15', 'mfi_15', 'avgvolm_15', 'avgvolty_15', 'rtrend_15', 'ci_15', 'return_25', 'gain_25', 'loss_25', 'ag_25', 'al_25', 'rs_25', 'rsi_25', 'updown_25', 'std_25', 'ma_25', 'mfi_25', 'avgvolm_25', 'avgvolty_25', 'rtrend_25', 'ci_25', 'return_50', 'gain_50', 'loss_50', 'ag_50', 'al_50', 'rs_50', 'rsi_50', 'updown_50', 'std_50', 'ma_50', 'mfi_50', 'avgvolm_50', 'avgvolty_50', 'rtrend_50', 'ci_50', 'return_100', 'gain_100', 'loss_100', 'ag_100', 'al_100', 'rs_100', 'rsi_100', 'updown_100', 'std_100', 'ma_100', 'mfi_100', 'avgvolm_100', 'avgvolty_100', 'rtrend_100', 'ci_100', 'mStateSt_2', 'mStateMid_2', 'mStateLt_2', 'mStateSt_5', 'mStateMid_5', 'mStateLt_5', 'mStateSt_10', 'mStateMid_10', 'mStateLt_10', 'mStateSt_15', 'mStateMid_15', 'mStateLt_15', 'mStateSt_25', 'mStateMid_25', 'mStateLt_25', 'mStateSt_50', 'mStateMid_50', 'mStateLt_50', 'mStateSt_100', 'mStateMid_100', 'mStateLt_100']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "next steps\n",
    "-merge the columns/format for all training data\n",
    "-label more data\n",
    "-add in funding rates, implied vols, spx, gold, qqq, silver, copper, tbill rate, fed decision day, money supply (M2)\n",
    "-stack the training data\n",
    "-set this up as training, validation, prediction\n",
    "-try LR, RF, and NN\n",
    "-research how to identify rangebound markets' peaks and troughs\n",
    "-figure out how to re-apply predicted markov states to be fed into training\n",
    "-run this three times: try both \n",
    "    1) predict short term, then 2) mid term then 3) long term\n",
    "    1) predict long term, then 2) mid term then 3) short term\n",
    "    and see which predicts better\n",
    "-apply Kelly Criterion\n",
    "-research stop losses\n",
    "\n",
    "\"\"\"\n",
    "#print(list(XRP_df.columns))\n",
    "#print(list(BTC_df.columns))\n",
    "#print(features_list)\n",
    "#print(type(features_list))\n",
    "#print(type(training_list))\n",
    "\n",
    "print(list(XRP_df.columns))\n",
    "print(list(BTC_df.columns))\n",
    "print(list(BNB_df.columns))\n",
    "print(list(DOGE_df.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = BTC_df.columns.tolist()\n",
    "BTC_df = BTC_df.reset_index(drop=True)\n",
    "XRP_df = XRP_df.reset_index(drop=True)\n",
    "BNB_df = BNB_df.reset_index(drop=True)\n",
    "DOGE_df = DOGE_df.reset_index(drop=True)\n",
    "result = pd.concat([BTC_df, XRP_df, BNB_df, DOGE_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     timestamp    open    high     low   close        volume    volume_ccy  \\\n",
      "0            0     1.0  4901.0     1.0  4901.0     19.260000  9.439326e+04   \n",
      "1            0  4901.0  4999.0  4790.0  4989.0      0.580096  2.894099e+03   \n",
      "2            0  4989.0  5922.0  4989.0  5741.6     20.739843  1.190799e+05   \n",
      "3            0  5741.6  5849.9  5473.7  5849.9     19.259580  1.126666e+05   \n",
      "4            0  5849.9  5849.9  5686.4  5848.0      8.658607  5.063553e+04   \n",
      "..         ...     ...     ...     ...     ...           ...           ...   \n",
      "196          0  9357.4  9758.3  8786.0  8963.0  11222.110840  1.047460e+08   \n",
      "197          0  8963.1  9178.0  8652.9  8800.8   8610.270343  7.624737e+07   \n",
      "198          0  8817.6  9386.8  8780.0  9236.6   7446.504090  6.853483e+07   \n",
      "199          0  9238.0  9429.9  8866.2  9357.7   7870.328640  7.223183e+07   \n",
      "200          0  9357.7  9566.0  9157.3  9299.7   8734.322895  8.185606e+07   \n",
      "\n",
      "      volccyquote  m_mt  m_st  ...  mStateLt_15  mStateSt_25  mStateMid_25  \\\n",
      "0    9.439326e+04     3     3  ...            0            0             0   \n",
      "1    2.894099e+03     3     3  ...            0            0             0   \n",
      "2    1.190799e+05     3     3  ...            0            0             0   \n",
      "3    1.126666e+05     3     3  ...            0            0             0   \n",
      "4    5.063553e+04     3     3  ...            0            0             0   \n",
      "..            ...   ...   ...  ...          ...          ...           ...   \n",
      "196  1.047460e+08     1     2  ...            1            1             1   \n",
      "197  7.624737e+07     1     2  ...            1            1             1   \n",
      "198  6.853483e+07     1     2  ...            1            1             1   \n",
      "199  7.223183e+07     1     2  ...            1            1             1   \n",
      "200  8.185606e+07     1     2  ...            1            1             1   \n",
      "\n",
      "     mStateLt_25  mStateSt_50  mStateMid_50  mStateLt_50  mStateSt_100  \\\n",
      "0              0            0             0            0             0   \n",
      "1              0            0             0            0             0   \n",
      "2              0            0             0            0             0   \n",
      "3              0            0             0            0             0   \n",
      "4              0            0             0            0             0   \n",
      "..           ...          ...           ...          ...           ...   \n",
      "196            1            1             1            1             1   \n",
      "197            1            1             1            1             1   \n",
      "198            1            1             1            1             1   \n",
      "199            1            1             1            1             1   \n",
      "200            1            1             1            1             1   \n",
      "\n",
      "     mStateMid_100  mStateLt_100  \n",
      "0                0             0  \n",
      "1                0             0  \n",
      "2                0             0  \n",
      "3                0             0  \n",
      "4                0             0  \n",
      "..             ...           ...  \n",
      "196              1             1  \n",
      "197              1             1  \n",
      "198              1             1  \n",
      "199              1             1  \n",
      "200              1             1  \n",
      "\n",
      "[201 rows x 151 columns]\n"
     ]
    }
   ],
   "source": [
    "#print(list(result.columns))\n",
    "#print(type(features_list))\n",
    "#print(features_list)\n",
    "#print(dataCleaned_mtup.head())\n",
    "\n",
    "str_to_int = {\n",
    "    \"utrending\": 3,\n",
    "    \"rangebound\": 2,\n",
    "    \"dtrending\": 1,\n",
    "    \"up\": 3,\n",
    "    \"flat\": 2,\n",
    "    \"down\": 1,    \n",
    "}\n",
    "\n",
    "def map_strings(x):\n",
    "    return str_to_int.get(x, 0)\n",
    "\n",
    "# Apply to all object (string) columns\n",
    "for col in result.select_dtypes(include='object').columns:\n",
    "    result[col] = result[col].apply(map_strings)\n",
    "\n",
    "# If you want to include NaN values as 0:\n",
    "result = result.fillna(0)\n",
    "print(result.head(201))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m_mt up\n",
    "features_list.extend(['m_mt', 'm_st', 'm_lt'])\n",
    "\n",
    "features_list_mtup = features_list.copy()\n",
    "features_list_mtup.extend(training_list)\n",
    "features_list_mtup.append('m_mt_up')\n",
    "drop_list_mtup = [item for item in all_cols if item not in features_list_mtup]\n",
    "\n",
    "dataCleaned_mtup = result.drop(drop_list_mtup, axis=1)\n",
    "dataCleaned_mtup = dataCleaned_mtup.dropna()\n",
    "\n",
    "y_mtup = dataCleaned_mtup['m_mt_up'] # Target variable\n",
    "X_mtup = dataCleaned_mtup.drop('m_mt_up', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m_mt flat\n",
    "features_list_mtfl = features_list.copy()\n",
    "features_list_mtfl.extend(training_list)\n",
    "features_list_mtfl.append('m_mt_flat')\n",
    "drop_list_mtfl = [item for item in all_cols if item not in features_list_mtfl]\n",
    "\n",
    "dataCleaned_mtfl = result.drop(drop_list_mtfl, axis=1)\n",
    "dataCleaned_mtfl = dataCleaned_mtfl.dropna()\n",
    "\n",
    "y_mtfl = dataCleaned_mtfl['m_mt_flat'] # Target variable\n",
    "X_mtfl = dataCleaned_mtfl.drop('m_mt_flat', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m_mt down\n",
    "features_list_mtdn = features_list.copy()\n",
    "features_list_mtdn.extend(training_list)\n",
    "features_list_mtdn.append('m_mt_down')\n",
    "drop_list_mtdn = [item for item in all_cols if item not in features_list_mtdn]\n",
    "\n",
    "dataCleaned_mtdn = result.drop(drop_list_mtdn, axis=1)\n",
    "dataCleaned_mtdn = dataCleaned_mtdn.dropna()\n",
    "\n",
    "y_mtdn = dataCleaned_mtdn['m_mt_down'] # Target variable\n",
    "X_mtdn = dataCleaned_mtdn.drop('m_mt_down', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=99999, random_state=42)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mtup, X_test_mtup, y_train_mtup, y_test_mtup = train_test_split(X_mtup, y_mtup, test_size=0.3, random_state=42)\n",
    "X_train_mtdn, X_test_mtdn, y_train_mtdn, y_test_mtdn = train_test_split(X_mtdn, y_mtdn, test_size=0.3, random_state=42)\n",
    "X_train_mtfl, X_test_mtfl, y_train_mtfl, y_test_mtfl = train_test_split(X_mtfl, y_mtfl, test_size=0.3, random_state=42)\n",
    "model_mtup = LogisticRegression(random_state=42, max_iter=99999) # Initialize the model\n",
    "model_mtdn = LogisticRegression(random_state=42, max_iter=99999) # Initialize the model\n",
    "model_mtfl = LogisticRegression(random_state=42, max_iter=99999) # Initialize the model\n",
    "model_mtup.fit(X_train_mtup, y_train_mtup) # Train the model\n",
    "model_mtdn.fit(X_train_mtdn, y_train_mtdn) # Train the model\n",
    "model_mtfl.fit(X_train_mtfl, y_train_mtfl) # Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['markov_mt', 'range', 'range%', 'obv', 'return', 'return_2', 'rs_2', 'updown_2', 'std_2', 'ma_2', 'mfi_2', 'avgvolm_2', 'avgvolty_2', 'rtrend_2', 'ci_2', 'return_15', 'rs_15', 'updown_15', 'std_15', 'ma_15', 'mfi_15', 'avgvolm_15', 'avgvolty_15', 'rtrend_15', 'ci_15', 'mStateSt_15', 'mStateMid_15', 'mStateLt_15', 'volCcyQuote']\n"
     ]
    }
   ],
   "source": [
    "print(list(dataCleaned.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy MT UP: 0.8200025192089684\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.86      0.90      7306\n",
      "         1.0       0.20      0.41      0.27       633\n",
      "\n",
      "    accuracy                           0.82      7939\n",
      "   macro avg       0.57      0.63      0.58      7939\n",
      "weighted avg       0.88      0.82      0.85      7939\n",
      "\n",
      "Accuracy MT DOWN: 0.8194986774152916\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.89      0.90      7018\n",
      "         1.0       0.25      0.27      0.26       921\n",
      "\n",
      "    accuracy                           0.82      7939\n",
      "   macro avg       0.57      0.58      0.58      7939\n",
      "weighted avg       0.83      0.82      0.82      7939\n",
      "\n",
      "Accuracy MT FLAT: 0.8589242977705001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.92      0.92      6956\n",
      "         1.0       0.43      0.41      0.42       983\n",
      "\n",
      "    accuracy                           0.86      7939\n",
      "   macro avg       0.67      0.67      0.67      7939\n",
      "weighted avg       0.86      0.86      0.86      7939\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification algorithms \n",
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\"\"\"\n",
    "y_pred_mtup = model_mtup.predict(X_test_mtup)\n",
    "y_pred_mtdn = model_mtdn.predict(X_test_mtdn)\n",
    "y_pred_mtfl = model_mtfl.predict(X_test_mtfl)\n",
    "\n",
    "print(\"Accuracy MT UP:\", accuracy_score(y_test_mtup, y_pred_mtup))\n",
    "print(classification_report(y_test_mtup, y_pred_mtup))\n",
    "\n",
    "print(\"Accuracy MT DOWN:\", accuracy_score(y_test_mtdn, y_pred_mtdn))\n",
    "print(classification_report(y_test_mtdn, y_pred_mtdn))\n",
    "\n",
    "print(\"Accuracy MT FLAT:\", accuracy_score(y_test_mtfl, y_pred_mtfl))\n",
    "print(classification_report(y_test_mtfl, y_pred_mtfl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1716631908.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.at[i, 'loss_'+str(period)] = 0\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1716631908.py:53: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.at[i, 'ag_'+str(period)] = up/period\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1716631908.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.at[i, 'al_'+str(period)] = down/period\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1716631908.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[rs_col] = np.nan#added\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1716631908.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[rsi_col] = np.nan    #added\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1716631908.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.at[i, 'updown_'+str(offset)] = -1\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1716631908.py:140: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['std_'+str(time)] = df['return_'+str(time)].rolling(time).std()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1716631908.py:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['ma_'+str(time)] = df['close'].rolling(time).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     timestamp    open     high     low   close       volume   volume_ccy  \\\n",
      "0            0  2.5000  10.0000  2.5000  3.7800  563244.8890  2119031.467   \n",
      "1            0  3.7859   3.8700  3.6055  3.8314  386915.6953  1445705.359   \n",
      "2            0  3.8310   3.8500  3.4601  3.6800  526700.5637  1898993.308   \n",
      "3            0  3.6709   4.0700  3.6368  3.8240  671441.0959  2596874.480   \n",
      "4            0  3.8049   3.9500  3.5802  3.8300  854647.4708  3260625.580   \n",
      "..         ...     ...      ...     ...     ...          ...          ...   \n",
      "196          0  5.9000   5.9374  5.7000  5.8298  825980.8914  4798303.945   \n",
      "197          0  5.8256   5.8851  5.7834  5.8079  489102.8176  2857223.020   \n",
      "198          0  5.8035   5.9284  5.7605  5.8954  423732.1717  2489875.764   \n",
      "199          0  5.8895   6.0475  5.8894  6.0098  418453.2806  2508701.179   \n",
      "200          0  6.0177   6.1030  5.9946  6.1028  234018.2803  1414737.353   \n",
      "\n",
      "     volccyquote  m_mt  m_st  ...    rs_100    rsi_100  updown_100   std_100  \\\n",
      "0    2119031.467     0     0  ...  0.000000   0.000000         0.0  0.000000   \n",
      "1    1445705.359     0     0  ...  0.000000   0.000000        -1.0  0.000000   \n",
      "2    1898993.308     3     3  ...  0.000000   0.000000        -1.0  0.000000   \n",
      "3    2596874.480     3     3  ...  0.000000   0.000000        -1.0  0.000000   \n",
      "4    3260625.580     3     3  ...  0.000000   0.000000        -1.0  0.000000   \n",
      "..           ...   ...   ...  ...       ...        ...         ...       ...   \n",
      "196  4798303.945     1     2  ...  1.134968  53.160895         1.0  0.000000   \n",
      "197  2857223.020     1     2  ...  1.129571  53.042190         1.0  0.000000   \n",
      "198  2489875.764     1     2  ...  1.148762  53.461579         1.0  0.000000   \n",
      "199  2508701.179     1     2  ...  1.174107  54.004094         1.0  0.164603   \n",
      "200  1414737.353     1     2  ...  1.194918  54.440212         1.0  0.166877   \n",
      "\n",
      "       ma_100    mfi_100   avgvolm_100  avgvolty_100  rtrend_100     ci_100  \n",
      "0    0.000000   0.000000  0.000000e+00           0.0         0.0   0.000000  \n",
      "1    0.000000   0.000000  0.000000e+00           0.0         0.0   0.000000  \n",
      "2    0.000000   0.000000  0.000000e+00           0.0         0.0   0.000000  \n",
      "3    0.000000   0.000000  0.000000e+00           0.0         0.0   0.000000  \n",
      "4    0.000000   0.000000  0.000000e+00           0.0         0.0   0.000000  \n",
      "..        ...        ...           ...           ...         ...        ...  \n",
      "196  6.052203  52.472261  3.787928e+06           0.0        94.0  57.224041  \n",
      "197  6.056032  51.817280  3.792473e+06           0.0        96.0  57.108467  \n",
      "198  6.061014  50.974622  3.772745e+06           0.0        98.0  57.109362  \n",
      "199  6.065360  51.739915  3.762064e+06           0.0       100.0  57.019159  \n",
      "200  6.066688  50.606543  3.701397e+06           0.0       100.0  56.687739  \n",
      "\n",
      "[201 rows x 130 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1716631908.py:142: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mfi_'+str(time)]= money_flow_index(df['high'], df['low'], df['close'], df['volume_ccy'], time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1716631908.py:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['avgvolm_'+str(time)] = df['volume_ccy'].rolling(time).mean()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1716631908.py:145: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['avgvolty_'+str(time)] = df['std_'+str(time)].rolling(time).mean()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1716631908.py:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['rtrend_'+str(time)] = df['updown_'+str(time)].rolling(time).sum()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_14068\\1716631908.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['ci_'+str(time)] = get_ci(df['high'], df['low'], df['close'], time)\n"
     ]
    }
   ],
   "source": [
    "DOT_df = pd.read_csv('2025-01-27-2017-09-07_DOT-USDT_1H_ohlc_M.csv')\n",
    "\n",
    "DOT_df.columns = DOT_df.columns.str.strip().str.lower()\n",
    "\n",
    "time_periods = (2,5,10,15,25,50,100)\n",
    "\n",
    "features_list_dot = []\n",
    "DOT_df, features_list_dot = feature_extraction(DOT_df, time_periods)\n",
    "\n",
    "str_to_int = {\n",
    "    \"utrending\": 3,\n",
    "    \"rangebound\": 2,\n",
    "    \"dtrending\": 1,\n",
    "    \"up\": 3,\n",
    "    \"flat\": 2,\n",
    "    \"down\": 1,    \n",
    "}\n",
    "\n",
    "def map_strings(x):\n",
    "    return str_to_int.get(x, 0)\n",
    "\n",
    "# Apply to all object (string) columns\n",
    "for col in DOT_df.select_dtypes(include='object').columns:\n",
    "    DOT_df[col] = DOT_df[col].apply(map_strings)\n",
    "\n",
    "# If you want to include NaN values as 0:\n",
    "DOT_df = DOT_df.fillna(0)\n",
    "print(DOT_df.head(201))\n",
    "\n",
    "#m_mt up\n",
    "features_list_dot.extend(['m_mt', 'm_st', 'm_lt'])\n",
    "\n",
    "features_list_mtup = features_list.copy()\n",
    "features_list_mtup.extend(training_list)\n",
    "features_list_mtup.append('m_mt_up')\n",
    "drop_list_mtup = [item for item in all_cols if item not in features_list_mtup]\n",
    "\n",
    "dataCleaned_mtup = DOT_df.drop(drop_list_mtup, axis=1)\n",
    "dataCleaned_mtup = dataCleaned_mtup.dropna()\n",
    "\n",
    "y_mtup = dataCleaned_mtup['m_mt_up'] # Target variable\n",
    "X_mtup = dataCleaned_mtup.drop('m_mt_up', axis=1)\n",
    "\n",
    "#m_mt flat\n",
    "features_list_mtfl = features_list.copy()\n",
    "features_list_mtfl.extend(training_list)\n",
    "features_list_mtfl.append('m_mt_flat')\n",
    "drop_list_mtfl = [item for item in all_cols if item not in features_list_mtfl]\n",
    "\n",
    "dataCleaned_mtfl = DOT_df.drop(drop_list_mtfl, axis=1)\n",
    "dataCleaned_mtfl = dataCleaned_mtfl.dropna()\n",
    "\n",
    "y_mtfl = dataCleaned_mtfl['m_mt_flat'] # Target variable\n",
    "X_mtfl = dataCleaned_mtfl.drop('m_mt_flat', axis=1)\n",
    "\n",
    "#m_mt down\n",
    "features_list_mtdn = features_list.copy()\n",
    "features_list_mtdn.extend(training_list)\n",
    "features_list_mtdn.append('m_mt_down')\n",
    "drop_list_mtdn = [item for item in all_cols if item not in features_list_mtdn]\n",
    "\n",
    "dataCleaned_mtdn = DOT_df.drop(drop_list_mtdn, axis=1)\n",
    "dataCleaned_mtdn = dataCleaned_mtdn.dropna()\n",
    "\n",
    "y_mtdn = dataCleaned_mtdn['m_mt_down'] # Target variable\n",
    "X_mtdn = dataCleaned_mtdn.drop('m_mt_down', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list_mtup = features_list.copy()\n",
    "features_list_mtup.extend(training_list)\n",
    "features_list_mtup.append('m_mt_up')\n",
    "drop_list_mtup = [item for item in all_cols if item not in features_list_mtup]\n",
    "\n",
    "#dataCleaned_mtup = DOT_df.drop(drop_list_mtup, axis=1)\n",
    "dataCleaned_mtup = dataCleaned_mtup.dropna()\n",
    "\n",
    "y_mtup = dataCleaned_mtup['m_mt_up'] # Target variable\n",
    "X_mtup = dataCleaned_mtup.drop('m_mt_up', axis=1)\n",
    "\n",
    "#m_mt flat\n",
    "features_list_mtfl = features_list.copy()\n",
    "features_list_mtfl.extend(training_list)\n",
    "features_list_mtfl.append('m_mt_flat')\n",
    "drop_list_mtfl = [item for item in all_cols if item not in features_list_mtfl]\n",
    "\n",
    "#dataCleaned_mtfl = DOT_df.drop(drop_list_mtfl, axis=1)\n",
    "dataCleaned_mtfl = dataCleaned_mtfl.dropna()\n",
    "\n",
    "y_mtfl = dataCleaned_mtfl['m_mt_flat'] # Target variable\n",
    "X_mtfl = dataCleaned_mtfl.drop('m_mt_flat', axis=1)\n",
    "\n",
    "#m_mt down\n",
    "features_list_mtdn = features_list.copy()\n",
    "features_list_mtdn.extend(training_list)\n",
    "features_list_mtdn.append('m_mt_down')\n",
    "drop_list_mtdn = [item for item in all_cols if item not in features_list_mtdn]\n",
    "\n",
    "#dataCleaned_mtdn = DOT_df.drop(drop_list_mtdn, axis=1)\n",
    "dataCleaned_mtdn = dataCleaned_mtdn.dropna()\n",
    "\n",
    "y_mtdn = dataCleaned_mtdn['m_mt_down'] # Target variable\n",
    "X_mtdn = dataCleaned_mtdn.drop('m_mt_down', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy MT UP: 0.8173916329692755\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.85      0.90     24315\n",
      "         1.0       0.20      0.40      0.26      2146\n",
      "\n",
      "    accuracy                           0.82     26461\n",
      "   macro avg       0.57      0.63      0.58     26461\n",
      "weighted avg       0.88      0.82      0.84     26461\n",
      "\n",
      "Accuracy MT DOWN: 0.8218132345716337\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.89      0.90     23460\n",
      "         1.0       0.24      0.26      0.25      3001\n",
      "\n",
      "    accuracy                           0.82     26461\n",
      "   macro avg       0.57      0.58      0.57     26461\n",
      "weighted avg       0.83      0.82      0.83     26461\n",
      "\n",
      "Accuracy MT FLAT: 0.8569970900570651\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.92      0.92     23083\n",
      "         1.0       0.44      0.40      0.42      3378\n",
      "\n",
      "    accuracy                           0.86     26461\n",
      "   macro avg       0.67      0.66      0.67     26461\n",
      "weighted avg       0.85      0.86      0.85     26461\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred_mtup = model_mtup.predict(X_mtup)\n",
    "y_pred_mtdn = model_mtdn.predict(X_mtdn)\n",
    "y_pred_mtfl = model_mtfl.predict(X_mtfl)\n",
    "\n",
    "print(\"Accuracy MT UP:\", accuracy_score(y_mtup, y_pred_mtup))\n",
    "print(classification_report(y_mtup, y_pred_mtup))\n",
    "\n",
    "print(\"Accuracy MT DOWN:\", accuracy_score(y_mtdn, y_pred_mtdn))\n",
    "print(classification_report(y_mtdn, y_pred_mtdn))\n",
    "\n",
    "print(\"Accuracy MT FLAT:\", accuracy_score(y_mtfl, y_pred_mtfl))\n",
    "print(classification_report(y_mtfl, y_pred_mtfl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
