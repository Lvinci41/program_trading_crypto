{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "379c8ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "from scipy.signal import argrelextrema\n",
    "import matplotlib as plot\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import itertools as it\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "\n",
    "#from keras.utils import to_categorical\n",
    "#from keras.models import Sequential\n",
    "\n",
    "enc = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43686908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import files \n",
    "#df = pd.read_csv('20250202-20170908_BTC-USDT_1D_okx_ohlc_ds.csv', skip_blank_lines=True)\n",
    "BTC_USDT_1d = pd.read_csv('20250202-20170908_BTC-USDT_1D.csv', skip_blank_lines=True)\n",
    "DOGEUSDT_1d = pd.read_csv('20250202-20170908_DOGEUSDT_1D.csv', skip_blank_lines=True)\n",
    "LEO_USDT_1h = pd.read_csv('20250202-20170908_LEO-USDT_1H.csv', skip_blank_lines=True)\n",
    "XRP_USDT_1d = pd.read_csv('20250127-20170907_XRP-USDT_1H.csv', skip_blank_lines=True)\n",
    "\n",
    "\n",
    "'''\n",
    "BTC_USDT_1d = pd.read_csv('20250202-20170908_BTC-USDT_1D_okx_ohlc_M.csv', skip_blank_lines=True)\n",
    "BNB_USDT_1h = pd.read_csv('20250127-20170907_BNB-USDT_1H_ohlc_M.csv', skip_blank_lines=True)\n",
    "DOGEUSDT_1d = pd.read_csv('20250202-20170908_DOGE-USDT_1D_okx_ohlc_M.csv', skip_blank_lines=True)\n",
    "XRP_USDT_1d = pd.read_csv('20250202-20170908_XRP-USDT_1D_okx_ohlc_M.csv', skip_blank_lines=True)\n",
    "DOT_USDT_1h = pd.read_csv('2025-01-27-2017-09-07_DOT-USDT_1H_ohlc_M.csv', skip_blank_lines=True)\n",
    "SHIBUSDT_1d = pd.read_csv('20250202-20170908_SHIB-USDT_1D_okx_ohlc_M.csv', skip_blank_lines=True)\n",
    "LEO_USDT_1h = pd.read_csv('20250202 22.18.32.-20170908 22.18.32._LEO-USDT_1H_okx_ohlc.csv', skip_blank_lines=True)\n",
    "\n",
    "\n",
    "20250202 22.18.32.-20170908 22.18.32._TRX-BTC_1H_okx_ohlc\n",
    "20250202 22.18.32.-20170908 22.18.32._TRX-USDC_1H_okx_ohlc\n",
    "20250202 22.18.32.-20170908 22.18.32._SOL-BTC_1H_okx_ohlc\n",
    "20250202 22.18.32.-20170908 22.18.32._SOL-DAI_1H_okx_ohlc\n",
    "20250202 22.18.32.-20170908 22.18.32._SOL-USDC_1H_okx_ohlc\n",
    "20250202 17.06.34.-20170908 17.06.34._ADA-USDT_1D_okx_ohlc\n",
    "20250202 17.06.34.-20170908 17.06.34._DOT-USDT_1D_okx_ohlc\n",
    "20250202 17.06.34.-20170908 17.06.34._LINK-USDT_1D_okx_ohlc\n",
    "20250202 17.06.34.-20170908 17.06.34._USDC-USDT_1D_okx_ohlc\n",
    "20250202 17.06.34.-20170908 17.06.34._DAI-USDT_1D_okx_ohlc\n",
    "20250202 17.06.34.-20170908 17.06.34._BCH-USDT_1D_okx_ohlc\n",
    "20250202 17.06.34.-20170908 17.06.34._BNB-USDT_1D_okx_ohlc\n",
    "20250202 12.58.42.-20170908 12.58.42._LTC-USDT_1D_okx_ohlc\n",
    "20250202 12.58.42.-20170908 12.58.42._ETH-USDT_1D_okx_ohlc\n",
    "20250202 12.58.42.-20170908 12.58.42._TRX-USDT_1D_okx_ohlc\n",
    "20250202 12.58.42.-20170908 12.58.42._SOL-USDT_1D_okx_ohlc\n",
    "2025-01-27 23.34.10.711000-2017-09-07 23.34.10.711000_BCH-USDT_1H_ohlc\n",
    "2025-01-27 23.34.10.711000-2017-09-07 23.34.10.711000_DOT-USDT_1H_ohlc\n",
    "2025-01-27 23.34.10.711000-2017-09-07 23.34.10.711000_XLM-USDT_1H_ohlc\n",
    "2025-01-27 23.34.10.711000-2017-09-07 23.34.10.711000_ADA-USDT_1H_ohlc\n",
    "2025-01-27 23.34.10.711000-2017-09-07 23.34.10.711000_LINK-USDT_1H_ohlc\n",
    "2025-01-27 23.34.10.711000-2017-09-07 23.34.10.711000_DAI-USDT_1H_ohlc\n",
    "2025-01-27 23.34.10.711000-2017-09-07 23.34.10.711000_USDC-USDT_1H_ohlc\n",
    "2025-01-27 23.34.10.711000-2017-09-07 23.34.10.711000_SHIB-USDT_1H_ohlc\n",
    "2025-01-27 23.34.10.711000-2017-09-07 23.34.10.711000_DOGE-USDT_1H_ohlc\n",
    "2025-01-27 23.12.01.675000-2017-09-07 23.12.01.675000_SUI-USDT_1H_ohlc\n",
    "2025-01-27 22.17.22.979000-2017-09-07 22.17.22.979000_XRP-USDT_1H_ohlc\n",
    "2025-01-27 22.11.54.226000-2017-09-07 22.11.54.226000_LTC-USDT_1H_ohlc\n",
    "2025-01-27 22.07.14.075000-2017-09-07 22.07.14.075000_ETH-USDT_1H_ohlc\n",
    "2025-01-27 21.15.46.583000-2017-09-07 21.15.46.583000_BTC-USDT_1H_ohlc\n",
    "'''\n",
    "\n",
    "#df_list = [BTC_USDT_1d, BNB_USDT_1h,DOGEUSDT_1d,XRP_USDT_1d,DOT_USDT_1h,SHIBUSDT_1d,LEO_USDT_1h]\n",
    "df_list = [BTC_USDT_1d,DOGEUSDT_1d,LEO_USDT_1h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83b0cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function declarations \n",
    "#RSI CALC\n",
    "def get_up_or_down(df, period):\n",
    "    for i in range(len(df)):\n",
    "        if i > 0:\n",
    "            if df.iloc[i]['close'] >= df.iloc[i-1]['close']:\n",
    "                df.at[i, 'gain_'+str(period)] = df.iloc[i]['close'] - df.iloc[i-1]['close']\n",
    "                df.at[i, 'loss_'+str(period)] = 0\n",
    "            elif df.iloc[i]['close'] < df.iloc[i-1]['close']:\n",
    "                df.at[i, 'loss_'+str(period)] = df.iloc[i-1]['close'] - df.iloc[i]['close']\n",
    "                df.at[i, 'gain_'+str(period)] = 0\n",
    "            else:\n",
    "                df.at[i, 'gain_'+str(period)] = 0\n",
    "                df.at[i, 'loss_'+str(period)] = 0\n",
    "    return df\n",
    "\n",
    "def get_up_or_down_bin(df, offset):\n",
    "    for i in range(len(df)):\n",
    "        if i > 0:\n",
    "            if df.iloc[i]['close'] >= df.iloc[i-offset]['close']:\n",
    "                df.at[i, 'updown_'+str(offset)] = 1\n",
    "            elif df.iloc[i]['close'] < df.iloc[i-offset]['close']:\n",
    "                df.at[i, 'updown_'+str(offset)] = -1                \n",
    "            else:\n",
    "                df.at[i, 'updown_'+str(offset)] = 0\n",
    "    return df\n",
    "  \n",
    "def get_relative_strength_index(df, period):\n",
    "    try:\n",
    "        df['Date'] = pd.to_datetime(df['timestamp'])\n",
    "    except:\n",
    "        df['Date'] = pd.to_datetime(df['timestamp'], format='%m/%d/%Y %I:%M:%S %p', errors='coerce')\n",
    "\n",
    "    df.set_index(df['Date'])\n",
    "    df = get_up_or_down(df, period)\n",
    "    return df\n",
    "\n",
    "def get_average_gains(df, period):\n",
    "    for i in range(len(df)):\n",
    "        n, up, down = 0, 0, 0\n",
    "        if i == period:\n",
    "            while n < period:\n",
    "                if df.iloc[i-n]['gain_'+str(period)] > 0:\n",
    "                    up += df.iloc[i-n]['gain_'+str(period)]\n",
    "                elif df.iloc[i-n]['loss_'+str(period)] > 0:\n",
    "                    down += df.iloc[i-n]['loss_'+str(period)]\n",
    "                else:\n",
    "                    up += 0\n",
    "                    down += 0\n",
    "                n += 1\n",
    "            df.at[i, 'ag_'+str(period)] = up/period\n",
    "            df.at[i, 'al_'+str(period)] = down/period\n",
    "        elif i > period:\n",
    "            df.at[i, 'ag_'+str(period)] = (df.iloc[i-1]['ag_'+str(period)] * (period - 1) + df.iloc[i]['gain_'+str(period)])/period\n",
    "            df.at[i, 'al_'+str(period)] = (df.iloc[i-1]['al_'+str(period)] * (period - 1) + df.iloc[i]['loss_'+str(period)])/period\n",
    "            df['ag_'+str(period)] = df['ag_'+str(period)].fillna(0)\n",
    "            df['al_'+str(period)] = df['al_'+str(period)].fillna(0)\n",
    "    return df\n",
    "\n",
    "def get_relative_strength(df, period):\n",
    "    df = get_relative_strength_index(df,period)\n",
    "    df = get_average_gains(df, period)\n",
    "    rs_col = f'rs_{period}'\n",
    "    rsi_col = f'rsi_{period}'\n",
    "    df[rs_col] = np.nan\n",
    "    df[rsi_col] = np.nan    \n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if i >= period and df.iloc[i]['al_'+str(period)] != 0:\n",
    "            df.at[i, rs_col] = df.iloc[i]['ag_'+str(period)] / df.iloc[i]['al_'+str(period)]\n",
    "            df.at[i, rsi_col] = 100 - (100 / (1 + df.at[i, rs_col]))\n",
    "        elif i >= period:\n",
    "            df.at[i, rs_col] = 0\n",
    "            df.at[i, rsi_col] = 100\n",
    "\n",
    "    return df\n",
    "\n",
    "##MONEY FLOW\n",
    "def get_typical_price(high, low, close):\n",
    "    typical_price = (high+low+close/3)\n",
    "    return typical_price\n",
    "\n",
    "def get_raw_money_flow(typical_price, volume):\n",
    "    money_flow = typical_price * volume\n",
    "    return money_flow\n",
    "\n",
    "def get_money_flow_ratio(money_flow, window=14):\n",
    "    signal = np.where(money_flow > money_flow.shift(1), 1, np.where(money_flow < money_flow.shift(1), -1, 0))\n",
    "    money_flow_s = money_flow * signal\n",
    "    \n",
    "    money_flow_positive = money_flow_s.rolling(window).apply(lambda x: np.sum(np.where(x >= 0.0, x, 0.0)), raw=True)\n",
    "    money_flow_negative = abs(money_flow_s.rolling(window).apply(lambda x: np.sum(np.where(x < 0.0, x, 0.0)), raw=True))\n",
    "    \n",
    "    money_flow_ratio = money_flow_positive / money_flow_negative\n",
    "    \n",
    "    return money_flow_ratio\n",
    "\n",
    "def get_money_flow_index(money_flow_ratio):\n",
    "    money_flow_index = 100. - 100./(1. + money_flow_ratio)\n",
    "    return money_flow_index\n",
    "\n",
    "def money_flow_index(high, low, close, volume, window=14):\n",
    "    mfr = get_money_flow_ratio((high+low+close/3) * volume, window)\n",
    "    mfi = 100. - 100./(1. + mfr)\n",
    "    return mfi\n",
    "\n",
    "#Choppiness index\n",
    "def get_ci(high, low, close, lookback):\n",
    "    tr1 = pd.DataFrame(high - low).rename(columns = {0:'tr1'})\n",
    "    tr2 = pd.DataFrame(abs(high - close.shift(1))).rename(columns = {0:'tr2'})\n",
    "    tr3 = pd.DataFrame(abs(low - close.shift(1))).rename(columns = {0:'tr3'})\n",
    "    frames = [tr1, tr2, tr3]\n",
    "    tr = pd.concat(frames, axis = 1, join = 'inner').dropna().max(axis = 1)\n",
    "    atr = tr.rolling(1).mean()\n",
    "    highh = high.rolling(lookback).max()\n",
    "    lowl = low.rolling(lookback).min()\n",
    "    ci = 100 * np.log10((atr.rolling(lookback).sum()) / (highh - lowl)) / np.log10(lookback)\n",
    "    return ci\n",
    "\n",
    "#Feature Extraction\n",
    "def feature_extraction(df, time_steps_arr):\n",
    "    df['range'] = df['high'] - df['low']\n",
    "    df['range%'] = df['range']/df['close']\n",
    "    df['obv'] = (np.sign(df['close'].diff()) * df['volume_ccy']).fillna(0).cumsum()    \n",
    "    df['return'] = df['close'].pct_change() \n",
    "    \n",
    "    features_list = ['range', 'range%', 'obv', 'return']\n",
    "    for time in time_steps_arr:\n",
    "        #df['return_'+str(time)] = (df.close / df.close.shift(time)) - 1\n",
    "        df['return_'+str(time)] = (df['close'] / df['close'].shift(time)) - 1\n",
    "        df = get_relative_strength(df, time)\n",
    "        df = get_up_or_down_bin(df, time)\n",
    "        df['std_'+str(time)] = df['return_'+str(time)].rolling(time).std()\n",
    "        df['ma_'+str(time)] = df['close'].rolling(time).mean()\n",
    "        df['mfi_'+str(time)]= money_flow_index(df['high'], df['low'], df['close'], df['volume_ccy'], time)\n",
    "        df['ma_'+str(time)] = df['close'].rolling(time).mean()\n",
    "        df['avgvolm_'+str(time)] = df['volume_ccy'].rolling(time).mean()\n",
    "        df['avgvolty_'+str(time)] = df['std_'+str(time)].rolling(time).mean()\n",
    "        df['rtrend_'+str(time)] = df['updown_'+str(time)].rolling(time).sum()\n",
    "        df['ci_'+str(time)] = get_ci(df['high'], df['low'], df['close'], time)\n",
    "        \n",
    "        features_list.extend(['return_' + str(time), 'rs_' + str(time), 'updown_' + str(time),'std_' + str(time), 'ma_' + str(time), 'mfi_' + str(time),'avgvolm_' + str(time), 'avgvolty_' + str(time), 'rtrend_' + str(time),'ci_' + str(time)])\n",
    "    return df, features_list\n",
    "\n",
    "def convert_markov_states(df):\n",
    "    for col_name, col_data in df.iteritems():\n",
    "        col_data = col_data.str.strip().str.lower()\n",
    "        for item in col_data:\n",
    "            if str(item) == \"dtrending\" or item == \"down\" or item == \"downward\":\n",
    "                item = 1                \n",
    "            elif item == \"rangebound\" or item == \"flat\" or item == \"stable\":\n",
    "                item = 2\n",
    "            elif item == \"utrending\" or item == \"up\" or item == \"upward\":\n",
    "                item = 3                \n",
    "            else:\n",
    "                item == 0        \n",
    "    return df\n",
    "                \n",
    "def training_states_shift(df, time_steps_arr):\n",
    "    for time in time_steps_arr:\n",
    "        df['mStateSt_'+str(time)] = df.m_st.shift(time)\n",
    "        df['mStateMid_'+str(time)] = df.m_mt.shift(time)\n",
    "        df['mStateLt_'+str(time)] = df.m_lt.shift(time)\n",
    "        \n",
    "        training_list = []\n",
    "        training_list.append('mStateSt_'+str(time))\n",
    "        training_list.append('mStateMid_'+str(time))\n",
    "        training_list.append('mStateLt_'+str(time))\n",
    "    return df, training_list\n",
    "\n",
    "def label_markov_states(\n",
    "    df,\n",
    "    price_col='close',\n",
    "    atr_window=30,\n",
    "    ma_window=50,\n",
    "    min_extrema_order=10,\n",
    "    max_extrema_order=30,\n",
    "    regression_window=30,\n",
    "    threshold_factor=1.25,\n",
    "    tol=0.25,\n",
    "    dbscan_eps=0.5,\n",
    "    dbscan_min_samples=2,\n",
    "    trend_p=0.05,\n",
    "    extrema_similarity_thresh=0.05\n",
    "):\n",
    "    df = df.copy()\n",
    "    prices = pd.to_numeric(df[price_col], errors='coerce').fillna(method='ffill').values\n",
    "    n = len(prices)\n",
    "\n",
    "    # Technical indicators\n",
    "    df['prev_close'] = df[price_col].shift(1)\n",
    "    df['TR'] = np.maximum(\n",
    "        df['high'] - df['low'],\n",
    "        np.abs(df['high'] - df['prev_close']),\n",
    "        np.abs(df['low'] - df['prev_close'])\n",
    "    )\n",
    "    df['ATR'] = df['TR'].rolling(atr_window).mean()\n",
    "    df['MA'] = df[price_col].rolling(ma_window).mean()\n",
    "    df['threshold'] = threshold_factor * df['ATR']\n",
    "    recent_vol = pd.Series(prices).rolling(window=atr_window, min_periods=10).std().fillna(method='bfill').values\n",
    "\n",
    "    states = np.full(n, np.nan, dtype=object)\n",
    "\n",
    "    # Windowed labeling with strict extremum start/end\n",
    "    for i in range(n - regression_window):\n",
    "        window = slice(i, i + regression_window)\n",
    "        window_prices = prices[window]\n",
    "        window_ma = df['MA'].iloc[window].values\n",
    "        window_atr = df['ATR'].iloc[i]\n",
    "        window_thresh = df['threshold'].iloc[i]\n",
    "\n",
    "        minima = argrelextrema(window_prices, np.less, order=3)[0]\n",
    "        maxima = argrelextrema(window_prices, np.greater, order=3)[0]\n",
    "        min_diff = np.std(window_prices[minima])/(window_atr+1e-8) if len(minima) > 1 else 0\n",
    "        max_diff = np.std(window_prices[maxima])/(window_atr+1e-8) if len(maxima) > 1 else 0\n",
    "        ma_crosses = len(np.where(np.diff(np.sign(window_prices - window_ma)))[0])\n",
    "        ma_flat = False\n",
    "        try:\n",
    "            slope_ma, _, _, ma_pvalue, _ = linregress(np.arange(ma_window), df['MA'].iloc[i:i+ma_window])\n",
    "            ma_flat = (abs(slope_ma) < (window_atr/ma_window)) and (ma_pvalue > 0.05)\n",
    "        except:\n",
    "            ma_flat = False\n",
    "\n",
    "        price_range = window_prices.max() - window_prices.min()\n",
    "\n",
    "        # Priority: Flat\n",
    "        if (\n",
    "            len(minima) >= 2 and len(maxima) >= 2 and\n",
    "            min_diff < 0.5 and max_diff < 0.5 and\n",
    "            ma_crosses >= (len(minima) + len(maxima)) and\n",
    "            ma_flat and price_range < window_thresh\n",
    "        ):\n",
    "            states[window] = 'flat'\n",
    "            continue\n",
    "\n",
    "        # Strict upward: must start at min, end at max, and no earlier price exceeds final price in this upward segment\n",
    "        if (window_prices[0] == np.min(window_prices)) and (window_prices[-1] == np.max(window_prices)):\n",
    "            if np.all(window_prices[:-1] <= window_prices[-1]):\n",
    "                pct_up = np.sum(np.diff(window_prices) > 0) / (regression_window - 1)\n",
    "                if pct_up >= (1 - tol):\n",
    "                    states[window] = 'up'\n",
    "                    continue\n",
    "        # Strict downward: must start at max, end at min, and no earlier price less than the final price in this downward segment\n",
    "        if (window_prices[0] == np.max(window_prices)) and (window_prices[-1] == np.min(window_prices)):\n",
    "            if np.all(window_prices[:-1] >= window_prices[-1]):\n",
    "                pct_down = np.sum(np.diff(window_prices) < 0) / (regression_window - 1)\n",
    "                if pct_down >= (1 - tol):\n",
    "                    states[window] = 'down'\n",
    "                    continue\n",
    "\n",
    "        # Regression slope and p-value\n",
    "        x = np.arange(regression_window)\n",
    "        slope, _, _, p_value, _ = linregress(x, window_prices)\n",
    "        slope_thr = window_atr / regression_window if not np.isnan(window_atr) else np.std(window_prices) / regression_window\n",
    "        if slope > slope_thr and p_value < trend_p:\n",
    "            # EXTREMUM ENFORCEMENT: Only assign upward if it terminates at a true local max (no earlier higher price in this upward run)\n",
    "            if window_prices[-1] == np.max(window_prices) and np.all(window_prices[:-1] <= window_prices[-1]):\n",
    "                states[window] = 'up'\n",
    "                continue\n",
    "        elif slope < -slope_thr and p_value < trend_p:\n",
    "            if window_prices[-1] == np.min(window_prices) and np.all(window_prices[:-1] >= window_prices[-1]):\n",
    "                states[window] = 'down'\n",
    "                continue\n",
    "\n",
    "    # Fuzzy DBSCAN clustering for extrema/state clean up\n",
    "    extrema_idx = set()\n",
    "    for order in range(min_extrema_order, max_extrema_order + 1):\n",
    "        min_idx = argrelextrema(prices, np.less_equal, order=order)[0]\n",
    "        max_idx = argrelextrema(prices, np.greater_equal, order=order)[0]\n",
    "        extrema_idx.update(min_idx)\n",
    "        extrema_idx.update(max_idx)\n",
    "    extrema_idx = np.array(sorted(list(extrema_idx)))\n",
    "    if len(extrema_idx) >= 2:\n",
    "        levels = prices[extrema_idx].reshape(-1, 1)\n",
    "        clusters = DBSCAN(eps=dbscan_eps * np.std(prices), min_samples=dbscan_min_samples).fit_predict(levels)\n",
    "        for ci in np.unique(clusters):\n",
    "            mask = clusters == ci\n",
    "            member_idx = extrema_idx[mask]\n",
    "            mmin, mmax = prices[member_idx].min(), prices[member_idx].max()\n",
    "            for idx in member_idx:\n",
    "                # Extremum-based enforcement: do not override strict logic!\n",
    "                if np.abs(prices[idx] - mmin) < 1e-8 and pd.isna(states[idx]):\n",
    "                    states[idx] = 'down'\n",
    "                elif np.abs(prices[idx] - mmax) < 1e-8 and pd.isna(states[idx]):\n",
    "                    states[idx] = 'up'\n",
    "\n",
    "    # Fill gaps with stable where price range is low\n",
    "    for i in range(n):\n",
    "        if pd.isna(states[i]):\n",
    "            lookback = min(regression_window, i)\n",
    "            lookahead = min(regression_window, n - i - 1)\n",
    "            window_prices = prices[i - lookback:i + lookahead + 1]\n",
    "            if len(window_prices) > 1 and (window_prices.max() - window_prices.min()) < df['threshold'].iloc[i]:\n",
    "                states[i] = 'flat'\n",
    "            else:\n",
    "                states[i] = states[i - 1] if i > 0 else 'flat'\n",
    "\n",
    "    # Post-processing: Relabel up/down chains as 'stable' if neighbor extrema are close\n",
    "    segments = []\n",
    "    start = 0\n",
    "    for i in range(1, n):\n",
    "        if states[i] != states[start]:\n",
    "            segments.append({'start': start, 'end': i - 1, 'label': states[start]})\n",
    "            start = i\n",
    "    segments.append({'start': start, 'end': n - 1, 'label': states[start]})\n",
    "\n",
    "    new_states = states.copy()\n",
    "    i = 0\n",
    "    while i < len(segments):\n",
    "        if segments[i]['label'] not in ('up', 'down'):\n",
    "            i += 1\n",
    "            continue\n",
    "        chain_mins, chain_maxs = [], []\n",
    "        j = i\n",
    "        while j < len(segments) and segments[j]['label'] in ('up', 'down'):\n",
    "            seg = segments[j]\n",
    "            seg_px = prices[seg['start']:seg['end'] + 1]\n",
    "            if seg['label'] == 'up':\n",
    "                chain_mins.append(seg_px[0])\n",
    "                chain_maxs.append(seg_px[-1])\n",
    "            else:\n",
    "                chain_maxs.append(seg_px[0])\n",
    "                chain_mins.append(seg_px[-1])\n",
    "            j += 1\n",
    "        if len(chain_mins) > 1 and len(chain_maxs) > 1:\n",
    "            mean_price = np.mean(prices[segments[i]['start']:segments[j - 1]['end'] + 1])\n",
    "            min_condition = all(\n",
    "                abs(chain_mins[k] - chain_mins[k + 1]) <= extrema_similarity_thresh * mean_price for k in range(len(chain_mins) - 1))\n",
    "            max_condition = all(\n",
    "                abs(chain_maxs[k] - chain_maxs[k + 1]) <= extrema_similarity_thresh * mean_price for k in range(len(chain_maxs) - 1))\n",
    "            if min_condition and max_condition:\n",
    "                rng = range(segments[i]['start'], segments[j - 1]['end'] + 1)\n",
    "                new_states[rng] = 'flat'\n",
    "        i = j\n",
    "\n",
    "    df['state'] = new_states\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6980846b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['timestamp', 'open', 'high', 'low', 'close', 'volume', 'volume_ccy', 'volCcyQuote', 'prev_close', 'TR', 'ATR', 'MA', 'threshold', 'state']   timestamp     open        high      low    close        volume  \\\n",
      "0   01:00.4  0.00285     0.00285  0.00285  0.00285  0.000000e+00   \n",
      "1   00:36.9  0.00348  1000.00000  0.00255  0.00302  7.121461e+07   \n",
      "2   00:13.3  0.00299     0.00324  0.00270  0.00303  2.688978e+07   \n",
      "3   59:49.8  0.00300     0.00311  0.00285  0.00300  8.136226e+06   \n",
      "4   59:26.2  0.00300     0.00315  0.00300  0.00300  6.847056e+06   \n",
      "\n",
      "     volume_ccy   volCcyQuote  prev_close         TR  ATR  MA  threshold state  \n",
      "0       0.00000       0.00000         NaN        NaN  NaN NaN        NaN  flat  \n",
      "1  224904.41800  224904.41800     0.00285  999.99745  NaN NaN        NaN  flat  \n",
      "2   81292.16987   81292.16987     0.00302    0.00054  NaN NaN        NaN  flat  \n",
      "3   24638.83890   24638.83890     0.00303    0.00026  NaN NaN        NaN  flat  \n",
      "4   21087.04477   21087.04477     0.00300    0.00015  NaN NaN        NaN  flat  \n"
     ]
    }
   ],
   "source": [
    "#label \n",
    "for i in range(len(df_list)):\n",
    "    df = df_list[i].copy()\n",
    "    df['close'] = pd.to_numeric(df['close'], errors='coerce')\n",
    "    #if f\"{df.name}[-1:]\"==\"h\":\n",
    "    df = label_markov_states(\n",
    "         df,\n",
    "         price_col='close',\n",
    "         atr_window=10, ma_window=10, min_extrema_order=3, max_extrema_order=10,\n",
    "         regression_window=100, threshold_factor=3.0, tol=0.75, dbscan_eps=1.0, dbscan_min_samples=2, trend_p=0.2,\n",
    "         extrema_similarity_thresh=0.05\n",
    "     )\n",
    "    df_list[i] = df  \n",
    "    \n",
    "    \n",
    "print(df_list[2].columns.tolist(), df_list[1].head())   \n",
    "#df_list = [BTC_USDT_1d,DOGEUSDT_1d,LEO_USDT_1h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3c00f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(LEO_USDT_1h.columns.tolist(), LEO_USDT_1h.head())   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebf539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph the state labels \n",
    "\n",
    "graph_df = df_list[1].set_index('state', append=True)['close']\n",
    "graph_df = graph_df.unstack('state')\n",
    "colors_map = {'flat': 'blue',\n",
    "              'down': 'red',\n",
    "              'up': 'green',\n",
    "              '0': 'black',\n",
    "             }\n",
    "colors_map[np.nan] = 'black'\n",
    "#BTC_mm_plot['date'] = BTCUSDT_data['Date']\n",
    "\n",
    "plot.rcParams['figure.figsize'] = [50, 50]\n",
    "graph_df.plot(color=colors_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9269040e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df_list[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c0194a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_87264\\3031403422.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.at[i, 'ag_'+str(period)] = up/period\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_87264\\3031403422.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.at[i, 'al_'+str(period)] = down/period\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_87264\\3031403422.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[rs_col] = np.nan\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_87264\\3031403422.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[rsi_col] = np.nan\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_87264\\3031403422.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.at[i, 'updown_'+str(offset)] = -1\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_87264\\3031403422.py:133: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['std_'+str(time)] = df['return_'+str(time)].rolling(time).std()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_87264\\3031403422.py:134: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['ma_'+str(time)] = df['close'].rolling(time).mean()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_87264\\3031403422.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mfi_'+str(time)]= money_flow_index(df['high'], df['low'], df['close'], df['volume_ccy'], time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_87264\\3031403422.py:137: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['avgvolm_'+str(time)] = df['volume_ccy'].rolling(time).mean()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_87264\\3031403422.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['avgvolty_'+str(time)] = df['std_'+str(time)].rolling(time).mean()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_87264\\3031403422.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['rtrend_'+str(time)] = df['updown_'+str(time)].rolling(time).sum()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_87264\\3031403422.py:140: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['ci_'+str(time)] = get_ci(df['high'], df['low'], df['close'], time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_87264\\3031403422.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.at[i, 'ag_'+str(period)] = up/period\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_87264\\3031403422.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.at[i, 'al_'+str(period)] = down/period\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_87264\\3031403422.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[rs_col] = np.nan\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_87264\\3031403422.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[rsi_col] = np.nan\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_87264\\3031403422.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.at[i, 'updown_'+str(offset)] = -1\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_87264\\3031403422.py:133: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['std_'+str(time)] = df['return_'+str(time)].rolling(time).std()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_87264\\3031403422.py:134: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['ma_'+str(time)] = df['close'].rolling(time).mean()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_87264\\3031403422.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['mfi_'+str(time)]= money_flow_index(df['high'], df['low'], df['close'], df['volume_ccy'], time)\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_87264\\3031403422.py:137: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['avgvolm_'+str(time)] = df['volume_ccy'].rolling(time).mean()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_87264\\3031403422.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['avgvolty_'+str(time)] = df['std_'+str(time)].rolling(time).mean()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_87264\\3031403422.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['rtrend_'+str(time)] = df['updown_'+str(time)].rolling(time).sum()\n",
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_87264\\3031403422.py:140: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['ci_'+str(time)] = get_ci(df['high'], df['low'], df['close'], time)\n"
     ]
    }
   ],
   "source": [
    "#assign features \n",
    "'''\n",
    "for i in range(len(df_list)):\n",
    "    df = df_list[i].copy()\n",
    "    df = df.columns.str.strip().str.lower()\n",
    "    #df = df\n",
    "    df_list[i] = df\n",
    "\n",
    "#df_list = [BTC_USDT_1d, BNB_USDT_1h,DOGEUSDT_1d,XRP_USDT_1d,DOT_USDT_1h,SHIBUSDT_1d,LEO_USDT_1h]\n",
    "df_list = [BTC_USDT_1d,DOGEUSDT_1d,LEO_USDT_1h]\n",
    "time_periods = (2,3,4,5,6,7,8,9,10,12,15,17,20,25,30,35,40,45,50,60,70,80,90,100,150,200)\n",
    "time_periods = (2,3,5,7,10,14,19,25,35,50,100,200)\n",
    "time_periods = (2,15,50,100)\n",
    "\n",
    "print(df_list[2].columns.tolist(), df_list[1].head()) \n",
    "'''\n",
    "time_periods = (2,5,10,15,25,50,100)\n",
    "\n",
    "features_list = []\n",
    "#TC_df, features_list = feature_extraction(BTC_df, time_periods)\n",
    "#BTC_df, training_list = training_states_shift(BTC_df, time_periods)\n",
    "\n",
    "for i in range(len(df_list)):\n",
    "    df = df_list[i].copy()    \n",
    "    if i == 0:\n",
    "        df, features_list = feature_extraction(df, time_periods)\n",
    "    else:\n",
    "        df = feature_extraction(df, time_periods)[0]\n",
    "    df_list[i] = df\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4465b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stack dfs \n",
    "all_cols = BTC_df.columns.tolist()\n",
    "\n",
    "for df in df_list:\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = convert_markov_states(df)\n",
    "\n",
    "result = pd.concat(df_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d228ca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update state names\n",
    "'''\n",
    "str_to_int = {\n",
    "    \"utrending\": 3,\n",
    "    \"rangebound\": 2,\n",
    "    \"dtrending\": 1,\n",
    "    \"up\": 3,\n",
    "    \"flat\": 2,\n",
    "    \"down\": 1,    \n",
    "    \"upward\": 3,\n",
    "    \"stable\": 2,\n",
    "    \"downward\": 1,       \n",
    "}\n",
    "\n",
    "def map_strings(x):\n",
    "    return str_to_int.get(x, 0)\n",
    "\n",
    "# Apply to all object (string) columns\n",
    "for col in result.select_dtypes(include='object').columns:\n",
    "    result[col] = result[col].apply(map_strings)\n",
    "\n",
    "# If you want to include NaN values as 0:\n",
    "result = result.fillna(0)\n",
    "print(result.head(201))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bf4712",
   "metadata": {},
   "outputs": [],
   "source": [
    "#m_mt up\n",
    "features_list.extend(['m_mt', 'm_st', 'm_lt'])\n",
    "\n",
    "features_list_mtup = features_list.copy()\n",
    "features_list_mtup.extend(training_list)\n",
    "features_list_mtup.append('m_mt_up')\n",
    "drop_list_mtup = [item for item in all_cols if item not in features_list_mtup]\n",
    "\n",
    "dataCleaned_mtup = result.drop(drop_list_mtup, axis=1)\n",
    "dataCleaned_mtup = dataCleaned_mtup.dropna()\n",
    "\n",
    "y_mtup = dataCleaned_mtup['m_mt_up'] # Target variable\n",
    "X_mtup = dataCleaned_mtup.drop('m_mt_up', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb514dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#m_mt flat\n",
    "features_list_mtfl = features_list.copy()\n",
    "features_list_mtfl.extend(training_list)\n",
    "features_list_mtfl.append('m_mt_flat')\n",
    "drop_list_mtfl = [item for item in all_cols if item not in features_list_mtfl]\n",
    "\n",
    "dataCleaned_mtfl = result.drop(drop_list_mtfl, axis=1)\n",
    "dataCleaned_mtfl = dataCleaned_mtfl.dropna()\n",
    "\n",
    "y_mtfl = dataCleaned_mtfl['m_mt_flat'] # Target variable\n",
    "X_mtfl = dataCleaned_mtfl.drop('m_mt_flat', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d3b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#m_mt down\n",
    "features_list_mtdn = features_list.copy()\n",
    "#features_list_mtdn.extend(training_list)\n",
    "features_list_mtdn.append('m_mt_down')\n",
    "drop_list_mtdn = [item for item in all_cols if item not in features_list_mtdn]\n",
    "\n",
    "dataCleaned_mtdn = result.drop(drop_list_mtdn, axis=1)\n",
    "dataCleaned_mtdn = dataCleaned_mtdn.dropna()\n",
    "\n",
    "y_mtdn = dataCleaned_mtdn['m_mt_down'] # Target variable\n",
    "X_mtdn = dataCleaned_mtdn.drop('m_mt_down', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ae8c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train logistic regression models for all three states\n",
    "X_train_mtup, X_test_mtup, y_train_mtup, y_test_mtup = train_test_split(X_mtup, y_mtup, test_size=0.3, random_state=42)\n",
    "X_train_mtdn, X_test_mtdn, y_train_mtdn, y_test_mtdn = train_test_split(X_mtdn, y_mtdn, test_size=0.3, random_state=42)\n",
    "X_train_mtfl, X_test_mtfl, y_train_mtfl, y_test_mtfl = train_test_split(X_mtfl, y_mtfl, test_size=0.3, random_state=42)\n",
    "model_mtup = LogisticRegression(random_state=42, max_iter=99999) # Initialize the model\n",
    "model_mtdn = LogisticRegression(random_state=42, max_iter=99999) # Initialize the model\n",
    "model_mtfl = LogisticRegression(random_state=42, max_iter=99999) # Initialize the model\n",
    "model_mtup.fit(X_train_mtup, y_train_mtup) # Train the model\n",
    "model_mtdn.fit(X_train_mtdn, y_train_mtdn) # Train the model\n",
    "model_mtfl.fit(X_train_mtfl, y_train_mtfl) # Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffa7520",
   "metadata": {},
   "outputs": [],
   "source": [
    "#m_mt flat, without state history\n",
    "'''\n",
    "features_list_mtfl = features_list.copy()\n",
    "features_list_mtfl.extend(training_list)\n",
    "features_list_mtfl.append('m_mt_flat')\n",
    "drop_list_mtfl = [item for item in all_cols if item not in features_list_mtfl]\n",
    "'''\n",
    "dataCleaned_mtfl2 = dataCleaned_mtfl.drop(training_list, axis=1)\n",
    "dataCleaned_mtfl2 = dataCleaned_mtfl2.dropna()\n",
    "\n",
    "y_mtfl2 = dataCleaned_mtfl2['m_mt_flat'] # Target variable\n",
    "X_mtfl2 = dataCleaned_mtfl2.drop('m_mt_flat', axis=1)\n",
    "\n",
    "X_train_mtfl2, X_test_mtfl2, y_train_mtfl2, y_test_mtfl2 = train_test_split(X_mtfl2, y_mtfl2, test_size=0.3, random_state=55)\n",
    "model_mtfl2 = LogisticRegression(random_state=42, max_iter=99999) # Initialize the model\n",
    "model_mtfl2.fit(X_train_mtfl2, y_train_mtfl2) # Train the model\n",
    "y_pred_mtfl2 = model_mtfl2.predict(X_test_mtfl2)\n",
    "print(\"Accuracy MT FLAT:\", accuracy_score(y_test_mtfl2, y_pred_mtfl2))\n",
    "print(classification_report(y_test_mtfl2, y_pred_mtfl2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69277051",
   "metadata": {},
   "outputs": [],
   "source": [
    "#explain the coefficients\n",
    "#print(list(dataCleaned_mtfl2.columns))\n",
    "\n",
    "coefficients = model_mtfl2.coef_[0] # For binary classification, coef_ is a 2D array, take the first row\n",
    "feature_names = X_mtfl2.columns\n",
    "\n",
    "print(\"Feature Weights (Coefficients):\")\n",
    "for feature, weight in zip(feature_names, coefficients):\n",
    "    print(f\"{feature}: {weight:.20f}\")\n",
    "\n",
    "# Access the intercept (bias term)\n",
    "intercept = model.intercept_[0] # For binary classification, intercept_ is a 1D array, take the first element\n",
    "print(f\"\\nIntercept: {intercept:.20f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fa90d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification algorithms \n",
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\"\"\"\n",
    "y_pred_mtup = model_mtup.predict(X_test_mtup)\n",
    "y_pred_mtdn = model_mtdn.predict(X_test_mtdn)\n",
    "y_pred_mtfl = model_mtfl.predict(X_test_mtfl)\n",
    "\n",
    "print(\"Accuracy MT UP:\", accuracy_score(y_test_mtup, y_pred_mtup))\n",
    "print(classification_report(y_test_mtup, y_pred_mtup))\n",
    "\n",
    "print(\"Accuracy MT DOWN:\", accuracy_score(y_test_mtdn, y_pred_mtdn))\n",
    "print(classification_report(y_test_mtdn, y_pred_mtdn))\n",
    "\n",
    "print(\"Accuracy MT FLAT:\", accuracy_score(y_test_mtfl, y_pred_mtfl))\n",
    "print(classification_report(y_test_mtfl, y_pred_mtfl))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
