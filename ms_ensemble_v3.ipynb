{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edc1063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy.signal import argrelextrema\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from hmmlearn import hmm\n",
    "import talib\n",
    "import re\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "import datetime\n",
    "\n",
    "\n",
    "#from pathlib import Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b53d7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ CONFIGURABLE PARAMETERS ------------\n",
    "N_MODELS = 3                                       # Number of MC-sim ensemble models\n",
    "TRAIN_PATH = '20250202-20170908_BTC-USDT_1D.csv'\n",
    "NEW_DATA_PATH = '20250127-20170907_XRP-USDT_1H.csv'\n",
    "DEFAULT_LOOKBACK_WINDOW = 20                        # Used for classic technical/feature rolling\n",
    "DEFAULT_VOL_WINDOW = 20                             # Volatility rolling window\n",
    "DEFAULT_CHOP_WINDOW = 20                            # Choppiness rolling window\n",
    "UPDOWN_PERIOD = 14                                  # For up/down period calculations\n",
    "UPDOWN_BIN_OFFSET = 1                               # For upbin result\n",
    "RSI_PERIOD = 14                                     # For custom RSI/gain/loss\n",
    "MFI_WINDOW = 14                                     # Money Flow Index window\n",
    "AVG_GAIN_WINDOW = 14                                # For avg gain/loss\n",
    "LAG_FEATURES = 3                                    # Number of lags to compute for each feature\n",
    "MA1 = 5                                             # Short moving avg window\n",
    "MA2 = 10                                            # Long moving avg window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7da117ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ FEATURE FUNCTIONS -------------\n",
    "def get_up_or_down(df, period):\n",
    "    df['gain_{}'.format(period)] = 0.0\n",
    "    df['loss_{}'.format(period)] = 0.0\n",
    "    for i in range(1, len(df)):\n",
    "        diff = df.iloc[i]['close'] - df.iloc[i-1]['close']\n",
    "        df.at[df.index[i], 'gain_{}'.format(period)] = max(diff, 0)\n",
    "        df.at[df.index[i], 'loss_{}'.format(period)] = max(-diff, 0)\n",
    "    return df\n",
    "\n",
    "def get_up_or_down_bin(df, offset):\n",
    "    df['updown_{}'.format(offset)] = 0\n",
    "    for i in range(offset, len(df)):\n",
    "        if df.iloc[i]['close'] > df.iloc[i-offset]['close']:\n",
    "            df.at[df.index[i], 'updown_{}'.format(offset)] = 1\n",
    "        elif df.iloc[i]['close'] < df.iloc[i-offset]['close']:\n",
    "            df.at[df.index[i], 'updown_{}'.format(offset)] = -1\n",
    "    return df\n",
    "\n",
    "def get_average_gains(df, period):\n",
    "    df['ag_{}'.format(period)] = 0.0\n",
    "    df['al_{}'.format(period)] = 0.0\n",
    "    for i in range(period, len(df)):\n",
    "        up = df['gain_{}'.format(period)].iloc[i-period+1:i+1].mean()\n",
    "        down = df['loss_{}'.format(period)].iloc[i-period+1:i+1].mean()\n",
    "        df.at[df.index[i], 'ag_{}'.format(period)] = up\n",
    "        df.at[df.index[i], 'al_{}'.format(period)] = down\n",
    "    return df\n",
    "\n",
    "def get_relative_strength(df, period):\n",
    "    df = get_up_or_down(df, period)\n",
    "    df = get_average_gains(df, period)\n",
    "    df['rs_{}'.format(period)] = 0.0\n",
    "    df['rsi_{}'.format(period)] = 0.0\n",
    "    for i in range(period, len(df)):\n",
    "        if df.at[df.index[i], 'al_{}'.format(period)] != 0:\n",
    "            rs = df.at[df.index[i], 'ag_{}'.format(period)] / df.at[df.index[i], 'al_{}'.format(period)]\n",
    "            df.at[df.index[i], 'rs_{}'.format(period)] = rs\n",
    "            df.at[df.index[i], 'rsi_{}'.format(period)] = 100 - (100 / (1 + rs))\n",
    "        else:\n",
    "            df.at[df.index[i], 'rs_{}'.format(period)] = 0\n",
    "            df.at[df.index[i], 'rsi_{}'.format(period)] = 100\n",
    "    return df\n",
    "\n",
    "def money_flow_index(df, window):\n",
    "    tp = (df['high'] + df['low'] + df['close']) / 3\n",
    "    rmf = tp * df['volume']\n",
    "    pos_mf = []\n",
    "    neg_mf = []\n",
    "    for i in range(1, len(df)):\n",
    "        if tp.iloc[i] > tp.iloc[i-1]:\n",
    "            pos_mf.append(rmf.iloc[i])\n",
    "            neg_mf.append(0)\n",
    "        elif tp.iloc[i] < tp.iloc[i-1]:\n",
    "            pos_mf.append(0)\n",
    "            neg_mf.append(rmf.iloc[i])\n",
    "        else:\n",
    "            pos_mf.append(0)\n",
    "            neg_mf.append(0)\n",
    "    pos_mf = pd.Series([0] + pos_mf, index=df.index)\n",
    "    neg_mf = pd.Series([0] + neg_mf, index=df.index)\n",
    "    pos_mf_rolling = pos_mf.rolling(window).sum()\n",
    "    neg_mf_rolling = neg_mf.rolling(window).sum()\n",
    "    mfr = pos_mf_rolling / (neg_mf_rolling + 1e-10)\n",
    "    mfi = 100 - 100 / (1 + mfr)\n",
    "    df['mfi_{}'.format(window)] = mfi\n",
    "    return df\n",
    "\n",
    "def get_ci(df, lookback):\n",
    "    tr1 = (df['high'] - df['low']).abs()\n",
    "    tr2 = (df['high'] - df['close'].shift(1)).abs()\n",
    "    tr3 = (df['low'] - df['close'].shift(1)).abs()\n",
    "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "    atr = tr.rolling(1).mean()\n",
    "    highh = df['high'].rolling(lookback).max()\n",
    "    lowl = df['low'].rolling(lookback).min()\n",
    "    ci = 100 * np.log10((atr.rolling(lookback).sum()) / (highh - lowl)) / np.log10(lookback)\n",
    "    df['ci_{}'.format(lookback)] = ci\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "728619a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# MARKET REGIME ANALYZER CLASS\n",
    "# ----------------------------\n",
    "class MarketRegimeAnalyzer:\n",
    "    def __init__(self, ohlc_df, lookback_window=20, volatility_threshold=0.5, chop_threshold=0.5):\n",
    "        self.df = self.normalize_column_names(ohlc_df.copy())\n",
    "        self.validate_input_columns()\n",
    "        self.lookback_window = lookback_window\n",
    "        self.volatility_threshold = volatility_threshold\n",
    "        self.chop_threshold = chop_threshold\n",
    "        self.state_labels = {\n",
    "            0: 'Rising',\n",
    "            1: 'Falling',\n",
    "            2: 'Steady',\n",
    "            3: 'Choppy',\n",
    "            4: 'No Label'\n",
    "        }\n",
    "\n",
    "    def normalize_column_names(self, df):\n",
    "        column_mapping = {}\n",
    "        for col in df.columns:\n",
    "            normalized = col.strip().lower()\n",
    "            normalized = re.sub(r'[^a-z0-9]', '', normalized)\n",
    "            column_mapping[col] = normalized\n",
    "        required_columns = {\n",
    "            'open': ['open', 'op', 'o'],\n",
    "            'high': ['high', 'hi', 'h'],\n",
    "            'low': ['low', 'lo', 'l'],\n",
    "            'close': ['close', 'cl', 'c', 'last'],\n",
    "            'volume': ['volume', 'vol', 'v', 'qty']\n",
    "        }\n",
    "        final_mapping = {}\n",
    "        available_columns = set(column_mapping.values())\n",
    "        for standard_name, variants in required_columns.items():\n",
    "            for variant in variants:\n",
    "                if variant in available_columns:\n",
    "                    for orig_col, normalized_col in column_mapping.items():\n",
    "                        if normalized_col == variant:\n",
    "                            final_mapping[standard_name] = orig_col\n",
    "                            break\n",
    "                    break\n",
    "        df = df.rename(columns={v: k for k, v in final_mapping.items()})\n",
    "        return df\n",
    "\n",
    "    def validate_input_columns(self):\n",
    "        required_columns = ['open', 'high', 'low', 'close', 'volume']\n",
    "        missing = [col for col in required_columns if col not in self.df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing required columns after normalization: {missing}\")\n",
    "        for col in required_columns:\n",
    "            self.df[col] = pd.to_numeric(self.df[col], errors='coerce')\n",
    "        self.df = self.df.dropna(subset=required_columns)\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        self.df['returns'] = self.df['close'].pct_change()\n",
    "        self.df['volatility'] = self.df['close'].rolling(window=self.lookback_window).std()\n",
    "        self.df['avg_volatility'] = self.df['volatility'].rolling(window=self.lookback_window).mean()\n",
    "        self.df['norm_volatility'] = self.df['volatility'] / self.df['avg_volatility']\n",
    "        self.df['sma'] = talib.SMA(self.df['close'], timeperiod=self.lookback_window)\n",
    "        self.df['ema'] = talib.EMA(self.df['close'], timeperiod=self.lookback_window)\n",
    "        self.df['adx'] = talib.ADX(self.df['high'], self.df['low'], self.df['close'], timeperiod=self.lookback_window)\n",
    "        self.df['rsi'] = talib.RSI(self.df['close'], timeperiod=self.lookback_window)\n",
    "        self.df['volume_sma'] = talib.SMA(self.df['volume'], timeperiod=self.lookback_window)\n",
    "        self.df['volume_change'] = self.df['volume'] / self.df['volume_sma']\n",
    "        self.df['local_min'] = self.df['close'] == self.df['close'].rolling(window=5, center=True).min()\n",
    "        self.df['local_max'] = self.df['close'] == self.df['close'].rolling(window=5, center=True).max()\n",
    "        self.df['atr'] = talib.ATR(self.df['high'], self.df['low'], self.df['close'], timeperiod=self.lookback_window)\n",
    "        # Choppiness index\n",
    "        self.df['chop'] = 100 * np.log10(self.df['atr'].rolling(window=self.lookback_window).sum() /\n",
    "              (self.df['high'].rolling(window=self.lookback_window).max() -\n",
    "               self.df['low'].rolling(window=self.lookback_window).min())) / np.log10(self.lookback_window)\n",
    "        self.df = self.df.dropna()\n",
    "\n",
    "    def label_states(self):\n",
    "        self.df['state'] = 4 # Default to 'No Label'\n",
    "        min_indices = argrelextrema(self.df['close'].values, np.less_equal, order=5)[0]\n",
    "        max_indices = argrelextrema(self.df['close'].values, np.greater_equal, order=5)[0]\n",
    "        for i in range(len(min_indices)-1):\n",
    "            start_idx = min_indices[i]\n",
    "            end_candidates = max_indices[max_indices > start_idx]\n",
    "            end_idx = end_candidates[0] if len(end_candidates) > 0 else len(self.df)-1\n",
    "            if all(self.df['close'].iloc[start_idx:end_idx+1] >= self.df['close'].iloc[start_idx]):\n",
    "                self.df.loc[self.df.index[start_idx]:self.df.index[end_idx],'state'] = 0\n",
    "        for i in range(len(max_indices)-1):\n",
    "            start_idx = max_indices[i]\n",
    "            end_candidates = min_indices[min_indices > start_idx]\n",
    "            end_idx = end_candidates[0] if len(end_candidates) > 0 else len(self.df)-1\n",
    "            if all(self.df['close'].iloc[start_idx:end_idx+1] <= self.df['close'].iloc[start_idx]):\n",
    "                self.df.loc[self.df.index[start_idx]:self.df.index[end_idx],'state'] = 1\n",
    "        steady_mask = (self.df['norm_volatility'] < self.volatility_threshold) & (self.df['state'] == 4)\n",
    "        self.df.loc[steady_mask, 'state'] = 2\n",
    "        chop_mask = (self.df['chop'] > self.chop_threshold) & (self.df['adx'] < 25) & (self.df['state'] == 4)\n",
    "        self.df.loc[chop_mask, 'state'] = 3\n",
    "\n",
    "    def prepare_model_data(self):\n",
    "        # Expanded feature list (using parameter variables)\n",
    "        feature_cols = [\n",
    "            'returns','volatility','norm_volatility','sma','ema','adx','rsi','volume_change','chop',\n",
    "            f'gain_{UPDOWN_PERIOD}',f'loss_{UPDOWN_PERIOD}',f'updown_{UPDOWN_BIN_OFFSET}',\n",
    "            f'ag_{AVG_GAIN_WINDOW}',f'al_{AVG_GAIN_WINDOW}',f'rs_{RSI_PERIOD}',f'rsi_{RSI_PERIOD}',\n",
    "            f'mfi_{MFI_WINDOW}',f'ci_{self.lookback_window}'\n",
    "        ]\n",
    "        # Generate lags and smoothers\n",
    "        for col in feature_cols:\n",
    "            if col in self.df.columns:\n",
    "                for lag in range(1, LAG_FEATURES + 1):\n",
    "                    self.df[f'{col}_lag{lag}'] = self.df[col].shift(lag)\n",
    "                self.df[f'{col}_ma{MA1}'] = self.df[col].rolling(MA1).mean()\n",
    "                self.df[f'{col}_ma{MA2}'] = self.df[col].rolling(MA2).mean()\n",
    "        self.df = self.df.dropna()\n",
    "        numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "        exclude_cols = ['state', 'local_min', 'local_max']\n",
    "        model_features = [col for col in numeric_cols if col not in exclude_cols]\n",
    "        self.X = self.df[model_features]\n",
    "        self.y = self.df['state']\n",
    "        self.model_features = model_features \n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.X, self.y, test_size=0.2, shuffle=False)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
    "        self.X_test_scaled = self.scaler.transform(self.X_test)\n",
    "\n",
    "    def train_models(self):\n",
    "        \"\"\"Train logistic regression, random forest, neural network, XGBoost, and SVM models. Return all models and their test set metrics.\"\"\"\n",
    "        label_keys = list(self.state_labels.keys())\n",
    "        label_names = list(self.state_labels.values())\n",
    "\n",
    "        # Logistic Regression\n",
    "        self.lr_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "        self.lr_model.fit(self.X_train_scaled, self.y_train)\n",
    "        lr_pred = self.lr_model.predict(self.X_test_scaled)\n",
    "        lr_report_str = classification_report(\n",
    "            self.y_test, lr_pred, labels=label_keys, target_names=label_names, zero_division=0\n",
    "        )\n",
    "        lr_report_dict = classification_report(\n",
    "            self.y_test, lr_pred, labels=label_keys, target_names=label_names, output_dict=True, zero_division=0\n",
    "        )\n",
    "        print(\"Logistic Regression Performance:\")\n",
    "        print(lr_report_str)\n",
    "\n",
    "        # Random Forest\n",
    "        self.rf_model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
    "        self.rf_model.fit(self.X_train, self.y_train)\n",
    "        rf_pred = self.rf_model.predict(self.X_test)\n",
    "        rf_report_str = classification_report(\n",
    "            self.y_test, rf_pred, labels=label_keys, target_names=label_names, zero_division=0\n",
    "        )\n",
    "        rf_report_dict = classification_report(\n",
    "            self.y_test, rf_pred, labels=label_keys, target_names=label_names, output_dict=True, zero_division=0\n",
    "        )\n",
    "        print(\"\\nRandom Forest Performance:\")\n",
    "        print(rf_report_str)\n",
    "\n",
    "        # Neural Network\n",
    "        self.nn_model = MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu',\n",
    "                                      solver='adam', max_iter=1000, random_state=42)\n",
    "        self.nn_model.fit(self.X_train_scaled, self.y_train)\n",
    "        nn_pred = self.nn_model.predict(self.X_test_scaled)\n",
    "        nn_report_str = classification_report(\n",
    "            self.y_test, nn_pred, labels=label_keys, target_names=label_names, zero_division=0\n",
    "        )\n",
    "        nn_report_dict = classification_report(\n",
    "            self.y_test, nn_pred, labels=label_keys, target_names=label_names, output_dict=True, zero_division=0\n",
    "        )\n",
    "        print(\"\\nNeural Network Performance:\")\n",
    "        print(nn_report_str)\n",
    "\n",
    "        # XGBoost/GBM\n",
    "        self.xgb_model = XGBClassifier(objective='multi:softmax', num_class=len(self.state_labels),\n",
    "                                       eval_metric='mlogloss', use_label_encoder=False, verbosity=0, random_state=42)\n",
    "        self.xgb_model.fit(self.X_train, self.y_train)\n",
    "        xgb_pred = self.xgb_model.predict(self.X_test)\n",
    "        xgb_report_str = classification_report(\n",
    "            self.y_test, xgb_pred, labels=label_keys, target_names=label_names, zero_division=0\n",
    "        )\n",
    "        xgb_report_dict = classification_report(\n",
    "            self.y_test, xgb_pred, labels=label_keys, target_names=label_names, output_dict=True, zero_division=0\n",
    "        )\n",
    "        print(\"\\nXGBoost Performance:\")\n",
    "        print(xgb_report_str)\n",
    "\n",
    "        # SVM (on scaled data, RBF kernel recommended for non-linearities)\n",
    "        self.svm_model = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "        self.svm_model.fit(self.X_train_scaled, self.y_train)\n",
    "        svm_pred = self.svm_model.predict(self.X_test_scaled)\n",
    "        svm_report_str = classification_report(\n",
    "            self.y_test, svm_pred, labels=label_keys, target_names=label_names, zero_division=0\n",
    "        )\n",
    "        svm_report_dict = classification_report(\n",
    "            self.y_test, svm_pred, labels=label_keys, target_names=label_names, output_dict=True, zero_division=0\n",
    "        )\n",
    "        print(\"\\nSVM (RBF) Performance:\")\n",
    "        print(svm_report_str)\n",
    "\n",
    "        self.train_stacking_ensemble(label_keys, label_names)\n",
    "\n",
    "        # Return all models and metrics\n",
    "        return {\n",
    "            \"models\": {\n",
    "                \"lr\": self.lr_model,\n",
    "                \"rf\": self.rf_model,\n",
    "                \"nn\": self.nn_model,\n",
    "                \"xgb\": self.xgb_model,\n",
    "                \"svm\": self.svm_model,\n",
    "            },\n",
    "            \"metrics\": {\n",
    "                \"lr\": {\n",
    "                    \"classification_report_str\": lr_report_str,\n",
    "                    \"classification_report_dict\": lr_report_dict,\n",
    "                },\n",
    "                \"rf\": {\n",
    "                    \"classification_report_str\": rf_report_str,\n",
    "                    \"classification_report_dict\": rf_report_dict,\n",
    "                },\n",
    "                \"nn\": {\n",
    "                    \"classification_report_str\": nn_report_str,\n",
    "                    \"classification_report_dict\": nn_report_dict,\n",
    "                },\n",
    "                \"xgb\": {\n",
    "                    \"classification_report_str\": xgb_report_str,\n",
    "                    \"classification_report_dict\": xgb_report_dict,\n",
    "                },\n",
    "                \"svm\": {\n",
    "                    \"classification_report_str\": svm_report_str,\n",
    "                    \"classification_report_dict\": svm_report_dict,\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def train_stacking_ensemble(self, label_keys, label_names):\n",
    "        base_estimators = [\n",
    "            ('rf', RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)),\n",
    "            ('lr', LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)),\n",
    "            ('mlp', MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', max_iter=1000, random_state=42)),\n",
    "            ('xgb', XGBClassifier(objective='multi:softmax', num_class=len(self.state_labels),\n",
    "                                  eval_metric='mlogloss', use_label_encoder=False, verbosity=0, random_state=42)),\n",
    "            ('svm', SVC(kernel='rbf', probability=True, random_state=42))\n",
    "        ]\n",
    "        meta_learner = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "        self.stacking_model = StackingClassifier(\n",
    "            estimators=base_estimators,\n",
    "            final_estimator=meta_learner,\n",
    "            cv=5,\n",
    "            passthrough=False,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        self.stacking_model.fit(self.X_train_scaled, self.y_train)\n",
    "        stacking_pred = self.stacking_model.predict(self.X_test_scaled)\n",
    "        print(\"\\nSTACKING ENSEMBLE PERFORMANCE:\")\n",
    "        # Always specify both labels and target_names (EXPLICIT FIX for your ValueError)\n",
    "        print(classification_report(\n",
    "            self.y_test, stacking_pred, labels=label_keys, target_names=label_names, zero_division=0\n",
    "        )) \n",
    "\n",
    "    def analyze_state_transitions(self):\n",
    "        self.hmm_model = hmm.CategoricalHMM(n_components=5, n_iter=100)\n",
    "        self.hmm_model.fit(self.y.values.reshape(-1, 1))\n",
    "        self.transition_matrix = self.hmm_model.transmat_\n",
    "\n",
    "    def run_analysis(self):\n",
    "        self.preprocess_data()\n",
    "        self.label_states()\n",
    "        self.prepare_model_data()\n",
    "        self.train_models()\n",
    "        self.analyze_state_transitions()\n",
    "\n",
    "# --------------------------\n",
    "# MONTE CARLO TRAINING SETUP\n",
    "# --------------------------\n",
    "def random_params():\n",
    "    lookback_window = random.choice([10, 15, 20, 25, 30])\n",
    "    volatility_threshold = round(random.uniform(0.3, 0.7), 2)\n",
    "    chop_threshold = round(random.uniform(0.3, 0.7), 2)\n",
    "    return lookback_window, volatility_threshold, chop_threshold\n",
    "\n",
    "def train_models_montecarlo(train_df, n_models=N_MODELS):\n",
    "    models = []\n",
    "    transition_matrices = []\n",
    "    model_params = []\n",
    "\n",
    "    for i in range(n_models):\n",
    "        lookback_window, volatility_threshold, chop_threshold = random_params()\n",
    "        analyzer = MarketRegimeAnalyzer(\n",
    "            train_df.copy(),\n",
    "            lookback_window=lookback_window,\n",
    "            volatility_threshold=volatility_threshold,\n",
    "            chop_threshold=chop_threshold\n",
    "        )\n",
    "        analyzer.run_analysis()\n",
    "        models.append(analyzer)\n",
    "        transition_matrices.append(analyzer.transition_matrix)\n",
    "        model_params.append({\n",
    "            'lookback_window': lookback_window,\n",
    "            'volatility_threshold': volatility_threshold,\n",
    "            'chop_threshold': chop_threshold\n",
    "        })\n",
    "        print(f\"Model {i+1}: window={lookback_window}, vol={volatility_threshold}, chop={chop_threshold}\")\n",
    "\n",
    "    return models, transition_matrices, model_params \n",
    "\n",
    "# --------------------------\n",
    "# ENSEMBLE APPLICATION BLOCK\n",
    "# --------------------------\n",
    "def ensemble_predict(models, new_data_path):\n",
    "    new_df_raw = pd.read_csv(new_data_path, parse_dates=[\"timestamp\"])\n",
    "    # Store each model's predictions\n",
    "    rf_preds, lr_preds, mlp_preds, xgb_preds, svm_preds = [], [], [], [], []\n",
    "    stacking_predictions = []\n",
    "    all_indices = []\n",
    "\n",
    "    for analyzer in models:\n",
    "        new_df = analyzer.normalize_column_names(new_df_raw.copy())\n",
    "        analyzer.df = new_df\n",
    "        analyzer.validate_input_columns()\n",
    "        analyzer.preprocess_data()\n",
    "\n",
    "        feature_cols = ['returns', 'volatility', 'norm_volatility', 'sma', 'ema',\n",
    "                        'adx', 'rsi', 'volume_change', 'chop']\n",
    "        for col in feature_cols:\n",
    "            for lag in range(1, 4):\n",
    "                analyzer.df[f'{col}_lag{lag}'] = analyzer.df[col].shift(lag)\n",
    "        for col in feature_cols:\n",
    "            analyzer.df[f'{col}_ma5'] = analyzer.df[col].rolling(5).mean()\n",
    "            analyzer.df[f'{col}_ma10'] = analyzer.df[col].rolling(10).mean()\n",
    "        analyzer.df = analyzer.df.dropna()\n",
    "\n",
    "        X_new = analyzer.df[analyzer.model_features]\n",
    "        X_new_scaled = analyzer.scaler.transform(X_new)\n",
    "\n",
    "        # Each model prediction\n",
    "        rf_preds.append(analyzer.rf_model.predict(X_new))\n",
    "        lr_preds.append(analyzer.lr_model.predict(X_new_scaled))\n",
    "        mlp_preds.append(analyzer.nn_model.predict(X_new_scaled))\n",
    "        xgb_preds.append(analyzer.xgb_model.predict(X_new))\n",
    "        svm_preds.append(analyzer.svm_model.predict(X_new_scaled))\n",
    "        all_indices.append(analyzer.df.index)\n",
    "\n",
    "        # stacking ensemble (just once, e.g. from the first analyzer)\n",
    "        if hasattr(analyzer, 'stacking_model') and len(stacking_predictions) == 0:\n",
    "            stacking_predictions = analyzer.stacking_model.predict(X_new_scaled)\n",
    "\n",
    "    # Use only the first model (or average/vote if you wish), here taking from the first ensemble member:\n",
    "    shared_indices = list(sorted(set.intersection(*[set(idx) for idx in all_indices])))\n",
    "    idx_map_rf = dict(zip(all_indices[0], rf_preds[0]))\n",
    "    idx_map_lr = dict(zip(all_indices[0], lr_preds[0]))\n",
    "    idx_map_mlp = dict(zip(all_indices[0], mlp_preds[0]))\n",
    "    idx_map_xgb = dict(zip(all_indices[0], xgb_preds[0]))\n",
    "    idx_map_svm = dict(zip(all_indices[0], svm_preds[0]))\n",
    "\n",
    "    rf_final = [idx_map_rf[i] for i in shared_indices]\n",
    "    lr_final = [idx_map_lr[i] for i in shared_indices]\n",
    "    mlp_final = [idx_map_mlp[i] for i in shared_indices]\n",
    "    xgb_final = [idx_map_xgb[i] for i in shared_indices]\n",
    "    svm_final = [idx_map_svm[i] for i in shared_indices]\n",
    "\n",
    "    if stacking_predictions is not None and len(stacking_predictions):\n",
    "        idx_map_stack = dict(zip(all_indices[0], stacking_predictions))\n",
    "        stack_final = [idx_map_stack[i] for i in shared_indices]\n",
    "    else:\n",
    "        stack_final = None\n",
    "\n",
    "    result_df = new_df_raw.loc[shared_indices].reset_index(drop=True)\n",
    "    # Add prediction columns to output\n",
    "    result_df['RF_Prediction'] = rf_final\n",
    "    result_df['LR_Prediction'] = lr_final\n",
    "    result_df['MLP_Prediction'] = mlp_final\n",
    "    result_df['XGB_Prediction'] = xgb_final\n",
    "    result_df['SVM_Prediction'] = svm_final\n",
    "    if stack_final is not None:\n",
    "        result_df['StackingPrediction'] = stack_final\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def markov_transition_matrix(labels):\n",
    "    \"\"\"\n",
    "    Empirical (count-based) transition probability matrix for a sequence of labels.\n",
    "    \"\"\"\n",
    "    states = pd.Series(labels).unique()\n",
    "    matrix = pd.DataFrame(0, index=states, columns=states, dtype=float)\n",
    "    for (a, b) in zip(labels[:-1], labels[1:]):\n",
    "        if (a in states) and (b in states):\n",
    "            matrix.loc[a, b] += 1\n",
    "    matrix = matrix.div(matrix.sum(axis=1), axis=0)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d28c507c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luke\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Rising       0.62      0.23      0.34       210\n",
      "     Falling       0.51      0.91      0.66       228\n",
      "      Steady       0.00      0.00      0.00         7\n",
      "      Choppy       0.00      0.00      0.00        34\n",
      "    No Label       0.51      0.42      0.46        48\n",
      "\n",
      "    accuracy                           0.53       527\n",
      "   macro avg       0.33      0.31      0.29       527\n",
      "weighted avg       0.52      0.53      0.46       527\n",
      "\n",
      "\n",
      "Random Forest Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Rising       0.61      0.26      0.36       210\n",
      "     Falling       0.48      0.93      0.63       228\n",
      "      Steady       0.00      0.00      0.00         7\n",
      "      Choppy       1.00      0.03      0.06        34\n",
      "    No Label       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.50       527\n",
      "   macro avg       0.42      0.24      0.21       527\n",
      "weighted avg       0.52      0.50      0.42       527\n",
      "\n",
      "\n",
      "Neural Network Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Rising       0.47      0.11      0.18       210\n",
      "     Falling       0.46      0.91      0.61       228\n",
      "      Steady       0.00      0.00      0.00         7\n",
      "      Choppy       0.29      0.12      0.17        34\n",
      "    No Label       0.22      0.04      0.07        48\n",
      "\n",
      "    accuracy                           0.45       527\n",
      "   macro avg       0.29      0.24      0.20       527\n",
      "weighted avg       0.42      0.45      0.35       527\n",
      "\n",
      "\n",
      "XGBoost Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Rising       0.59      0.20      0.30       210\n",
      "     Falling       0.46      0.88      0.61       228\n",
      "      Steady       0.00      0.00      0.00         7\n",
      "      Choppy       0.00      0.00      0.00        34\n",
      "    No Label       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.46       527\n",
      "   macro avg       0.21      0.22      0.18       527\n",
      "weighted avg       0.44      0.46      0.38       527\n",
      "\n",
      "\n",
      "SVM (RBF) Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Rising       0.58      0.25      0.35       210\n",
      "     Falling       0.48      0.91      0.63       228\n",
      "      Steady       0.00      0.00      0.00         7\n",
      "      Choppy       0.00      0.00      0.00        34\n",
      "    No Label       0.88      0.15      0.25        48\n",
      "\n",
      "    accuracy                           0.51       527\n",
      "   macro avg       0.39      0.26      0.25       527\n",
      "weighted avg       0.52      0.51      0.43       527\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luke\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STACKING ENSEMBLE PERFORMANCE:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Rising       0.71      0.34      0.46       210\n",
      "     Falling       0.49      0.91      0.63       228\n",
      "      Steady       0.00      0.00      0.00         7\n",
      "      Choppy       0.00      0.00      0.00        34\n",
      "    No Label       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.53       527\n",
      "   macro avg       0.24      0.25      0.22       527\n",
      "weighted avg       0.49      0.53      0.46       527\n",
      "\n",
      "Model 1: window=15, vol=0.41, chop=0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luke\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Rising       0.61      0.22      0.33       210\n",
      "     Falling       0.52      0.90      0.66       230\n",
      "      Steady       0.35      0.44      0.39        16\n",
      "      Choppy       0.06      0.06      0.06        16\n",
      "    No Label       0.46      0.11      0.17        57\n",
      "\n",
      "    accuracy                           0.51       529\n",
      "   macro avg       0.40      0.35      0.32       529\n",
      "weighted avg       0.53      0.51      0.45       529\n",
      "\n",
      "\n",
      "Random Forest Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Rising       0.61      0.38      0.46       210\n",
      "     Falling       0.52      0.89      0.65       230\n",
      "      Steady       0.00      0.00      0.00        16\n",
      "      Choppy       0.00      0.00      0.00        16\n",
      "    No Label       0.00      0.00      0.00        57\n",
      "\n",
      "    accuracy                           0.54       529\n",
      "   macro avg       0.22      0.25      0.22       529\n",
      "weighted avg       0.47      0.54      0.47       529\n",
      "\n",
      "\n",
      "Neural Network Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Rising       0.47      0.23      0.31       210\n",
      "     Falling       0.49      0.87      0.62       230\n",
      "      Steady       1.00      0.12      0.22        16\n",
      "      Choppy       0.00      0.00      0.00        16\n",
      "    No Label       0.33      0.02      0.03        57\n",
      "\n",
      "    accuracy                           0.47       529\n",
      "   macro avg       0.46      0.25      0.24       529\n",
      "weighted avg       0.46      0.47      0.40       529\n",
      "\n",
      "\n",
      "XGBoost Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Rising       0.67      0.31      0.42       210\n",
      "     Falling       0.51      0.89      0.65       230\n",
      "      Steady       0.33      0.19      0.24        16\n",
      "      Choppy       0.20      0.19      0.19        16\n",
      "    No Label       0.00      0.00      0.00        57\n",
      "\n",
      "    accuracy                           0.52       529\n",
      "   macro avg       0.34      0.31      0.30       529\n",
      "weighted avg       0.50      0.52      0.46       529\n",
      "\n",
      "\n",
      "SVM (RBF) Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Rising       0.62      0.30      0.40       210\n",
      "     Falling       0.48      0.89      0.62       230\n",
      "      Steady       0.00      0.00      0.00        16\n",
      "      Choppy       0.00      0.00      0.00        16\n",
      "    No Label       0.25      0.02      0.03        57\n",
      "\n",
      "    accuracy                           0.51       529\n",
      "   macro avg       0.27      0.24      0.21       529\n",
      "weighted avg       0.48      0.51      0.44       529\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luke\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STACKING ENSEMBLE PERFORMANCE:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Rising       0.59      0.45      0.51       210\n",
      "     Falling       0.51      0.81      0.63       230\n",
      "      Steady       1.00      0.12      0.22        16\n",
      "      Choppy       0.00      0.00      0.00        16\n",
      "    No Label       0.00      0.00      0.00        57\n",
      "\n",
      "    accuracy                           0.54       529\n",
      "   macro avg       0.42      0.28      0.27       529\n",
      "weighted avg       0.49      0.54      0.48       529\n",
      "\n",
      "Model 2: window=10, vol=0.58, chop=0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luke\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Rising       0.62      0.23      0.34       210\n",
      "     Falling       0.51      0.91      0.66       228\n",
      "      Steady       0.00      0.00      0.00         2\n",
      "      Choppy       0.25      0.03      0.05        36\n",
      "    No Label       0.51      0.39      0.44        51\n",
      "\n",
      "    accuracy                           0.53       527\n",
      "   macro avg       0.38      0.31      0.30       527\n",
      "weighted avg       0.54      0.53      0.47       527\n",
      "\n",
      "\n",
      "Random Forest Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Rising       0.58      0.25      0.35       210\n",
      "     Falling       0.49      0.93      0.64       228\n",
      "      Steady       0.00      0.00      0.00         2\n",
      "      Choppy       1.00      0.03      0.05        36\n",
      "    No Label       0.00      0.00      0.00        51\n",
      "\n",
      "    accuracy                           0.50       527\n",
      "   macro avg       0.41      0.24      0.21       527\n",
      "weighted avg       0.51      0.50      0.42       527\n",
      "\n",
      "\n",
      "Neural Network Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Rising       0.47      0.10      0.16       210\n",
      "     Falling       0.46      0.91      0.61       228\n",
      "      Steady       0.00      0.00      0.00         2\n",
      "      Choppy       0.23      0.08      0.12        36\n",
      "    No Label       0.56      0.18      0.27        51\n",
      "\n",
      "    accuracy                           0.46       527\n",
      "   macro avg       0.34      0.25      0.23       527\n",
      "weighted avg       0.45      0.46      0.36       527\n",
      "\n",
      "\n",
      "XGBoost Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Rising       0.63      0.18      0.28       210\n",
      "     Falling       0.47      0.91      0.62       228\n",
      "      Steady       0.00      0.00      0.00         2\n",
      "      Choppy       0.05      0.03      0.04        36\n",
      "    No Label       0.00      0.00      0.00        51\n",
      "\n",
      "    accuracy                           0.47       527\n",
      "   macro avg       0.23      0.22      0.19       527\n",
      "weighted avg       0.46      0.47      0.38       527\n",
      "\n",
      "\n",
      "SVM (RBF) Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Rising       0.58      0.25      0.35       210\n",
      "     Falling       0.48      0.91      0.63       228\n",
      "      Steady       0.00      0.00      0.00         2\n",
      "      Choppy       0.00      0.00      0.00        36\n",
      "    No Label       0.88      0.14      0.24        51\n",
      "\n",
      "    accuracy                           0.51       527\n",
      "   macro avg       0.39      0.26      0.24       527\n",
      "weighted avg       0.53      0.51      0.44       527\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luke\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STACKING ENSEMBLE PERFORMANCE:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Rising       0.66      0.34      0.45       210\n",
      "     Falling       0.49      0.89      0.63       228\n",
      "      Steady       0.00      0.00      0.00         2\n",
      "      Choppy       0.00      0.00      0.00        36\n",
      "    No Label       0.00      0.00      0.00        51\n",
      "\n",
      "    accuracy                           0.52       527\n",
      "   macro avg       0.23      0.25      0.22       527\n",
      "weighted avg       0.47      0.52      0.45       527\n",
      "\n",
      "Model 3: window=15, vol=0.33, chop=0.66\n",
      "\n",
      "HMM State Transition Matrix:\n",
      "          Rising  Falling  Steady  Choppy  No Label\n",
      "Rising     0.779    0.000   0.001   0.012     0.208\n",
      "Falling    0.000    0.838   0.134   0.016     0.013\n",
      "Steady     0.200    0.000   0.791   0.009     0.000\n",
      "Choppy     0.000    0.000   0.040   0.923     0.037\n",
      "No Label   0.000    0.955   0.037   0.007     0.001\n",
      "\n",
      "Empirical Transition Matrix (markov_transition_matrix):\n",
      "          Rising  Falling  No Label  Choppy  Steady\n",
      "Rising     0.866    0.120     0.007   0.007   0.001\n",
      "Falling    0.099    0.891     0.004   0.006   0.000\n",
      "No Label   0.027    0.053     0.904   0.005   0.011\n",
      "Choppy     0.053    0.020     0.033   0.888   0.007\n",
      "Steady     0.000    0.100     0.100   0.200   0.600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_2024\\1667118612.py:306: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  new_df_raw = pd.read_csv(new_data_path, parse_dates=[\"timestamp\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   timestamp     open     high      low    close         volume   \n",
      "0    59:54.8  1.98290  1.98290  1.95999  1.96989     842.147582  \\\n",
      "1    01:04.8  1.95999  1.98690  1.94989  1.96490   20430.103160   \n",
      "2    00:03.7  1.96189  2.02850  1.95510  2.01350   35270.421280   \n",
      "3    59:02.7  2.00999  2.08000  2.00999  2.06000   34362.915080   \n",
      "4    00:12.7  2.05900  2.10000  1.92999  2.06510   98407.296190   \n",
      "5    59:11.6  2.06510  2.09100  2.05719  2.08429  102571.693700   \n",
      "6    00:21.6  2.08000  2.08740  2.02309  2.05869  128110.300800   \n",
      "7    59:20.6  2.05060  2.06099  2.04170  2.04300   79484.232720   \n",
      "8    00:30.6  2.04610  2.05410  2.02009  2.04919   66181.477600   \n",
      "9    59:29.5  2.03239  2.04939  2.01020  2.03549   43167.292400   \n",
      "10   00:39.6  2.03549  2.10000  2.02510  2.08199   30226.535200   \n",
      "11   59:38.5  2.08199  2.10000  2.05010  2.08000   44310.176800   \n",
      "12   00:48.5  2.07549  2.08000  2.04459  2.05759   22563.294760   \n",
      "13   59:47.5  2.05140  2.07880  2.04510  2.06199   13757.502240   \n",
      "14   00:57.5  2.06210  2.06210  2.02009  2.04289   51073.295380   \n",
      "15   59:56.4  2.04139  2.06490  2.04000  2.05989   34683.591380   \n",
      "16   58:55.4  2.06199  2.06479  2.02000  2.03609   45599.544730   \n",
      "17   00:05.4  2.03560  2.04219  2.00000  2.01290   59994.866940   \n",
      "18   59:04.3  2.01279  2.02490  1.96049  2.00729   58013.808330   \n",
      "19   00:14.3  2.00729  2.00729  1.94070  1.95540  131937.346400   \n",
      "20   59:13.3  1.95369  1.99910  1.94409  1.98889   99975.467490   \n",
      "21   00:23.3  1.98889  1.99889  1.97799  1.98259   45264.602270   \n",
      "22   59:22.2  1.98130  1.98259  1.95760  1.96059   62125.448040   \n",
      "23   00:32.3  1.96059  1.97789  1.95799  1.96009   23323.780610   \n",
      "24   59:31.2  1.96189  1.98370  1.95910  1.97599   10020.650470   \n",
      "25   00:41.2  1.97589  2.00190  1.97589  2.00010   15984.740770   \n",
      "26   59:40.2  2.00190  2.01370  1.99869  1.99930   54460.341110   \n",
      "27   00:50.2  1.99380  1.99990  1.94029  1.96389   31167.034420   \n",
      "28   59:49.1  1.96389  1.98130  1.94900  1.96570   17797.567790   \n",
      "29   00:59.1  1.96789  1.96789  1.90999  1.94569   18803.939330   \n",
      "\n",
      "       volume_ccy    volCcyQuote  RF_Prediction  LR_Prediction   \n",
      "0     1654.572261    1654.572261              0              1  \\\n",
      "1    39916.375480   39916.375480              0              1   \n",
      "2    70261.494640   70261.494640              0              0   \n",
      "3    70152.163160   70152.163160              0              0   \n",
      "4   201500.961900  201500.961900              0              0   \n",
      "5   212245.359300  212245.359300              0              0   \n",
      "6   263317.737200  263317.737200              1              0   \n",
      "7   163048.772200  163048.772200              0              1   \n",
      "8   134925.958900  134925.958900              0              1   \n",
      "9    87486.444250   87486.444250              0              1   \n",
      "10   62654.938600   62654.938600              0              0   \n",
      "11   92024.869080   92024.869080              0              0   \n",
      "12   46528.407330   46528.407330              0              1   \n",
      "13   28295.172060   28295.172060              0              1   \n",
      "14  104606.354400  104606.354400              0              1   \n",
      "15   71248.388270   71248.388270              0              0   \n",
      "16   93117.512810   93117.512810              0              1   \n",
      "17  121373.496000  121373.496000              0              1   \n",
      "18  115316.802400  115316.802400              0              1   \n",
      "19  260965.555900  260965.555900              3              1   \n",
      "20  197242.831900  197242.831900              3              0   \n",
      "21   89957.326000   89957.326000              3              1   \n",
      "22  122287.184700  122287.184700              3              1   \n",
      "23   45892.437760   45892.437760              3              1   \n",
      "24   19731.082320   19731.082320              0              1   \n",
      "25   31815.618340   31815.618340              0              0   \n",
      "26  108960.738500  108960.738500              0              0   \n",
      "27   61174.685540   61174.685540              3              1   \n",
      "28   34960.688460   34960.688460              0              1   \n",
      "29   36586.811750   36586.811750              0              1   \n",
      "\n",
      "    MLP_Prediction  XGB_Prediction  SVM_Prediction  StackingPrediction  \n",
      "0                1               1               0                   1  \n",
      "1                1               1               0                   1  \n",
      "2                0               0               0                   0  \n",
      "3                0               0               0                   0  \n",
      "4                0               0               0                   0  \n",
      "5                0               0               0                   0  \n",
      "6                0               1               0                   0  \n",
      "7                0               1               1                   1  \n",
      "8                0               1               1                   1  \n",
      "9                0               1               1                   1  \n",
      "10               0               2               0                   0  \n",
      "11               0               0               0                   0  \n",
      "12               0               0               0                   1  \n",
      "13               4               0               0                   0  \n",
      "14               0               0               1                   1  \n",
      "15               1               0               1                   0  \n",
      "16               1               0               1                   1  \n",
      "17               1               0               1                   1  \n",
      "18               1               1               1                   1  \n",
      "19               0               0               1                   1  \n",
      "20               0               0               0                   0  \n",
      "21               0               1               0                   1  \n",
      "22               0               1               1                   1  \n",
      "23               0               1               0                   1  \n",
      "24               1               1               0                   0  \n",
      "25               1               0               0                   0  \n",
      "26               1               1               0                   0  \n",
      "27               1               1               1                   1  \n",
      "28               1               1               1                   0  \n",
      "29               1               1               1                   1  \n"
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "# MAIN BLOCK\n",
    "# ----------\n",
    "if __name__ == \"__main__\":\n",
    "    # Train ensemble of models\n",
    "    train_df = pd.read_csv(TRAIN_PATH, parse_dates=[\"timestamp\"])\n",
    "    models, trans_matrices, params = train_models_montecarlo(train_df, n_models=N_MODELS)\n",
    "\n",
    "    # Take the transition matrix from the first model for demonstration\n",
    "    analyzer = models[0]  # e.g., \"best\" or first model\n",
    "    print(\"\\nHMM State Transition Matrix:\")\n",
    "    hmm_mat = pd.DataFrame(analyzer.transition_matrix,\n",
    "                           index=list(analyzer.state_labels.values()),\n",
    "                           columns=list(analyzer.state_labels.values()))\n",
    "    print(hmm_mat.round(3))\n",
    "\n",
    "    print(\"\\nEmpirical Transition Matrix (markov_transition_matrix):\")\n",
    "    emp_mat = markov_transition_matrix(analyzer.y.values)\n",
    "    emp_mat.index = [analyzer.state_labels.get(x, x) for x in emp_mat.index]\n",
    "    emp_mat.columns = [analyzer.state_labels.get(x, x) for x in emp_mat.columns]\n",
    "    print(emp_mat.round(3))    \n",
    "    \n",
    "    # Apply all models to unseen data for ensemble prediction output\n",
    "    final_df = ensemble_predict(models, NEW_DATA_PATH)\n",
    "\n",
    "    print(final_df.head(30))\n",
    "    # (Optional) Save results\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    outname = f\"markov_labeled_result_{timestamp}.csv\"\n",
    "    outdir = './dir'\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "\n",
    "    fullname = os.path.join(outdir, outname)    \n",
    "\n",
    "    final_df.to_csv(fullname, index=False)  \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f641f098",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fc55f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e284fe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44574d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9890560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
