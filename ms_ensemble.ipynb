{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b64b467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy.signal import argrelextrema\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from hmmlearn import hmm\n",
    "import talib\n",
    "import re\n",
    "import os\n",
    "#from pathlib import Path\n",
    "\n",
    "# --------------------\n",
    "# CONFIGURATION\n",
    "# --------------------\n",
    "N_MODELS = 3  # Change for desired Monte Carlo iterations\n",
    "TRAIN_PATH = '20250202-20170908_BTC-USDT_1D.csv'  # Update paths as needed!\n",
    "NEW_DATA_PATH = '20250127-20170907_XRP-USDT_1H.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e15bb74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# MARKET REGIME ANALYZER CLASS\n",
    "# ----------------------------\n",
    "\n",
    "class MarketRegimeAnalyzer:\n",
    "    def __init__(self, ohlc_df, lookback_window=20, volatility_threshold=0.5, chop_threshold=0.5):\n",
    "        self.df = self.normalize_column_names(ohlc_df.copy())\n",
    "        self.validate_input_columns()\n",
    "        self.lookback_window = lookback_window\n",
    "        self.volatility_threshold = volatility_threshold\n",
    "        self.chop_threshold = chop_threshold\n",
    "        self.state_labels = {\n",
    "            0: 'Rising',\n",
    "            1: 'Falling',\n",
    "            2: 'Steady',\n",
    "            3: 'Choppy',\n",
    "            4: 'No Label'\n",
    "        }\n",
    "\n",
    "    def normalize_column_names(self, df):\n",
    "        column_mapping = {}\n",
    "        for col in df.columns:\n",
    "            normalized = col.strip().lower()\n",
    "            normalized = re.sub(r'[^a-z0-9]', '', normalized)\n",
    "            column_mapping[col] = normalized\n",
    "        required_columns = {\n",
    "            'open': ['open', 'op', 'o'],\n",
    "            'high': ['high', 'hi', 'h'],\n",
    "            'low': ['low', 'lo', 'l'],\n",
    "            'close': ['close', 'cl', 'c', 'last'],\n",
    "            'volume': ['volume', 'vol', 'v', 'qty']\n",
    "        }\n",
    "        final_mapping = {}\n",
    "        available_columns = set(column_mapping.values())\n",
    "        for standard_name, variants in required_columns.items():\n",
    "            for variant in variants:\n",
    "                if variant in available_columns:\n",
    "                    for orig_col, normalized_col in column_mapping.items():\n",
    "                        if normalized_col == variant:\n",
    "                            final_mapping[standard_name] = orig_col\n",
    "                            break\n",
    "                    break\n",
    "        df = df.rename(columns={v: k for k, v in final_mapping.items()})\n",
    "        return df\n",
    "\n",
    "    def validate_input_columns(self):\n",
    "        required_columns = ['open', 'high', 'low', 'close', 'volume']\n",
    "        missing = [col for col in required_columns if col not in self.df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing required columns after normalization: {missing}\")\n",
    "        for col in required_columns:\n",
    "            self.df[col] = pd.to_numeric(self.df[col], errors='coerce')\n",
    "        self.df = self.df.dropna(subset=required_columns)\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        self.df['returns'] = self.df['close'].pct_change()\n",
    "        self.df['volatility'] = self.df['close'].rolling(window=self.lookback_window).std()\n",
    "        self.df['avg_volatility'] = self.df['volatility'].rolling(window=self.lookback_window).mean()\n",
    "        self.df['norm_volatility'] = self.df['volatility'] / self.df['avg_volatility']\n",
    "        self.df['sma'] = talib.SMA(self.df['close'], timeperiod=self.lookback_window)\n",
    "        self.df['ema'] = talib.EMA(self.df['close'], timeperiod=self.lookback_window)\n",
    "        self.df['adx'] = talib.ADX(self.df['high'], self.df['low'], self.df['close'], timeperiod=self.lookback_window)\n",
    "        self.df['rsi'] = talib.RSI(self.df['close'], timeperiod=self.lookback_window)\n",
    "        self.df['volume_sma'] = talib.SMA(self.df['volume'], timeperiod=self.lookback_window)\n",
    "        self.df['volume_change'] = self.df['volume'] / self.df['volume_sma']\n",
    "        self.df['local_min'] = self.df['close'] == self.df['close'].rolling(window=5, center=True).min()\n",
    "        self.df['local_max'] = self.df['close'] == self.df['close'].rolling(window=5, center=True).max()\n",
    "        self.df['atr'] = talib.ATR(self.df['high'], self.df['low'], self.df['close'], timeperiod=self.lookback_window)\n",
    "        self.df['chop'] = 100 * np.log10(self.df['atr'].rolling(window=self.lookback_window).sum() /\n",
    "                                         (self.df['high'].rolling(window=self.lookback_window).max() -\n",
    "                                          self.df['low'].rolling(window=self.lookback_window).min())) / np.log10(self.lookback_window)\n",
    "        self.df = self.df.dropna()\n",
    "\n",
    "    def label_states(self):\n",
    "        self.df['state'] = 4  # Default to 'No Label'\n",
    "        min_indices = argrelextrema(self.df['close'].values, np.less_equal, order=5)[0]\n",
    "        max_indices = argrelextrema(self.df['close'].values, np.greater_equal, order=5)[0]\n",
    "        for i in range(len(min_indices)-1):\n",
    "            start_idx = min_indices[i]\n",
    "            end_candidates = max_indices[max_indices > start_idx]\n",
    "            end_idx = end_candidates[0] if len(end_candidates) > 0 else len(self.df)-1\n",
    "            if all(self.df['close'].iloc[start_idx:end_idx+1] >= self.df['close'].iloc[start_idx]):\n",
    "                self.df.loc[self.df.index[start_idx]:self.df.index[end_idx],'state'] = 0\n",
    "        for i in range(len(max_indices)-1):\n",
    "            start_idx = max_indices[i]\n",
    "            end_candidates = min_indices[min_indices > start_idx]\n",
    "            end_idx = end_candidates[0] if len(end_candidates) > 0 else len(self.df)-1\n",
    "            if all(self.df['close'].iloc[start_idx:end_idx+1] <= self.df['close'].iloc[start_idx]):\n",
    "                self.df.loc[self.df.index[start_idx]:self.df.index[end_idx],'state'] = 1\n",
    "        steady_mask = (self.df['norm_volatility'] < self.volatility_threshold) & (self.df['state'] == 4)\n",
    "        self.df.loc[steady_mask, 'state'] = 2\n",
    "        chop_mask = (self.df['chop'] > self.chop_threshold) & (self.df['adx'] < 25) & (self.df['state'] == 4)\n",
    "        self.df.loc[chop_mask, 'state'] = 3\n",
    "\n",
    "    def prepare_model_data(self):\n",
    "        feature_cols = ['returns', 'volatility', 'norm_volatility', 'sma', 'ema',\n",
    "                        'adx', 'rsi', 'volume_change', 'chop']\n",
    "        for col in feature_cols:\n",
    "            for lag in range(1, 4):\n",
    "                self.df[f'{col}_lag{lag}'] = self.df[col].shift(lag)\n",
    "        for col in feature_cols:\n",
    "            self.df[f'{col}_ma5'] = self.df[col].rolling(5).mean()\n",
    "            self.df[f'{col}_ma10'] = self.df[col].rolling(10).mean()\n",
    "        self.df = self.df.dropna()\n",
    "        numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "        exclude_cols = ['state', 'local_min', 'local_max']\n",
    "        model_features = [col for col in numeric_cols if col not in exclude_cols]\n",
    "        self.X = self.df[model_features]\n",
    "        self.y = self.df['state']\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.X, self.y, test_size=0.2, shuffle=False)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
    "        self.X_test_scaled = self.scaler.transform(self.X_test)\n",
    "\n",
    "    def train_models(self):\n",
    "        self.lr_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "        self.lr_model.fit(self.X_train_scaled, self.y_train)\n",
    "        self.rf_model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
    "        self.rf_model.fit(self.X_train, self.y_train)\n",
    "        self.nn_model = MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu',\n",
    "                                      solver='adam', max_iter=1000, random_state=42)\n",
    "        self.nn_model.fit(self.X_train_scaled, self.y_train)\n",
    "\n",
    "    def analyze_state_transitions(self):\n",
    "        self.hmm_model = hmm.CategoricalHMM(n_components=5, n_iter=100)\n",
    "        self.hmm_model.fit(self.y.values.reshape(-1, 1))\n",
    "        self.transition_matrix = self.hmm_model.transmat_\n",
    "\n",
    "    def run_analysis(self):\n",
    "        self.preprocess_data()\n",
    "        self.label_states()\n",
    "        self.prepare_model_data()\n",
    "        self.train_models()\n",
    "        self.analyze_state_transitions()\n",
    "        \n",
    "# --------------------------\n",
    "# MONTE CARLO TRAINING SETUP\n",
    "# --------------------------\n",
    "\n",
    "def random_params():\n",
    "    lookback_window = random.choice([10, 15, 20, 25, 30])\n",
    "    volatility_threshold = round(random.uniform(0.3, 0.7), 2)\n",
    "    chop_threshold = round(random.uniform(0.3, 0.7), 2)\n",
    "    return lookback_window, volatility_threshold, chop_threshold\n",
    "\n",
    "def train_models_montecarlo(train_df, n_models=N_MODELS):\n",
    "    models = []\n",
    "    transition_matrices = []\n",
    "    model_params = []\n",
    "\n",
    "    for i in range(n_models):\n",
    "        lookback_window, volatility_threshold, chop_threshold = random_params()\n",
    "        analyzer = MarketRegimeAnalyzer(\n",
    "            train_df.copy(),\n",
    "            lookback_window=lookback_window,\n",
    "            volatility_threshold=volatility_threshold,\n",
    "            chop_threshold=chop_threshold\n",
    "        )\n",
    "        analyzer.run_analysis()\n",
    "        models.append(analyzer)\n",
    "        transition_matrices.append(analyzer.transition_matrix)\n",
    "        model_params.append({\n",
    "            'lookback_window': lookback_window,\n",
    "            'volatility_threshold': volatility_threshold,\n",
    "            'chop_threshold': chop_threshold\n",
    "        })\n",
    "        print(f\"Model {i+1}: window={lookback_window}, vol={volatility_threshold}, chop={chop_threshold}\")\n",
    "\n",
    "    return models, transition_matrices, model_params        \n",
    "\n",
    "# --------------------------\n",
    "# ENSEMBLE APPLICATION BLOCK\n",
    "# --------------------------\n",
    "\n",
    "def ensemble_predict(models, new_data_path):\n",
    "    new_df_raw = pd.read_csv(new_data_path, parse_dates=[\"timestamp\"])\n",
    "    all_predictions = []\n",
    "    indices_keep = None\n",
    "\n",
    "    for i, analyzer in enumerate(models):\n",
    "        new_df = analyzer.normalize_column_names(new_df_raw.copy())\n",
    "        analyzer.df = new_df\n",
    "        analyzer.validate_input_columns()\n",
    "        analyzer.preprocess_data()\n",
    "\n",
    "        feature_cols = ['returns', 'volatility', 'norm_volatility', 'sma', 'ema',\n",
    "                        'adx', 'rsi', 'volume_change', 'chop']\n",
    "        for col in feature_cols:\n",
    "            for lag in range(1, 4):\n",
    "                analyzer.df[f'{col}_lag{lag}'] = analyzer.df[col].shift(lag)\n",
    "        for col in feature_cols:\n",
    "            analyzer.df[f'{col}_ma5'] = analyzer.df[col].rolling(5).mean()\n",
    "            analyzer.df[f'{col}_ma10'] = analyzer.df[col].rolling(10).mean()\n",
    "        analyzer.df = analyzer.df.dropna()\n",
    "\n",
    "        numeric_cols = analyzer.df.select_dtypes(include=['float64', 'int64']).columns\n",
    "        exclude_cols = ['state', 'local_min', 'local_max']\n",
    "        model_features = [col for col in numeric_cols if col not in exclude_cols]\n",
    "        X_new = analyzer.df[model_features]\n",
    "        X_new_scaled = analyzer.scaler.transform(X_new)\n",
    "        preds = analyzer.rf_model.predict(X_new)  # Use RF by default, can switch to nn_model.predict(X_new_scaled)\n",
    "\n",
    "        all_predictions.append(preds)\n",
    "        if indices_keep is None:\n",
    "            indices_keep = analyzer.df.index\n",
    "\n",
    "    import numpy as np\n",
    "    predictions_arr = np.vstack(all_predictions)\n",
    "\n",
    "    # For each time step, calculate probabilities across models\n",
    "    prediction_summary = []\n",
    "    state_labels = models[0].state_labels\n",
    "    for i in range(predictions_arr.shape[1]):\n",
    "        state_vec = predictions_arr[:, i]\n",
    "        state_counts = dict(zip(*np.unique(state_vec, return_counts=True)))\n",
    "        total = np.sum(list(state_counts.values()))\n",
    "        regime_probs = {state_labels[s]: state_counts.get(s, 0)/total for s in state_labels}\n",
    "        prediction_summary.append(regime_probs)\n",
    "\n",
    "    summary_df = pd.DataFrame(prediction_summary)\n",
    "    result_df = new_df_raw.loc[indices_keep].reset_index(drop=True)\n",
    "    final_df = pd.concat([result_df[['timestamp','open','high','low','close','volume']].reset_index(drop=True), summary_df], axis=1)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4261da04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luke\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: window=15, vol=0.69, chop=0.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luke\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2: window=30, vol=0.67, chop=0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luke\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3: window=25, vol=0.52, chop=0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luke\\AppData\\Local\\Temp\\ipykernel_46784\\1037687188.py:177: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  new_df_raw = pd.read_csv(new_data_path, parse_dates=[\"timestamp\"])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 64290 and the array at index 1 has size 64260",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m models, trans_matrices, params \u001b[38;5;241m=\u001b[39m train_models_montecarlo(train_df, n_models\u001b[38;5;241m=\u001b[39mN_MODELS)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Apply all models to unseen data for ensemble prediction output\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m final_df \u001b[38;5;241m=\u001b[39m \u001b[43mensemble_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNEW_DATA_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(final_df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m30\u001b[39m))\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# (Optional) Save results\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 209\u001b[0m, in \u001b[0;36mensemble_predict\u001b[1;34m(models, new_data_path)\u001b[0m\n\u001b[0;32m    206\u001b[0m         indices_keep \u001b[38;5;241m=\u001b[39m analyzer\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mindex\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m predictions_arr \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_predictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;66;03m# For each time step, calculate probabilities across models\u001b[39;00m\n\u001b[0;32m    212\u001b[0m prediction_summary \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\core\\shape_base.py:296\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    295\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 64290 and the array at index 1 has size 64260"
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "# MAIN BLOCK\n",
    "# ----------\n",
    "if __name__ == \"__main__\":\n",
    "    # Train ensemble of models\n",
    "    train_df = pd.read_csv(TRAIN_PATH, parse_dates=[\"timestamp\"])\n",
    "    models, trans_matrices, params = train_models_montecarlo(train_df, n_models=N_MODELS)\n",
    "\n",
    "    # Apply all models to unseen data for ensemble prediction output\n",
    "    final_df = ensemble_predict(models, NEW_DATA_PATH)\n",
    "\n",
    "    print(final_df.head(30))\n",
    "    # (Optional) Save results\n",
    "    \n",
    "    outname = 'name.csv'\n",
    "    outdir = './dir'\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "\n",
    "    fullname = os.path.join(outdir, outname)    \n",
    "\n",
    "    final_df.to_csv(fullname, index=False)  \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd174c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29e855a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbab602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f176b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde661b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
